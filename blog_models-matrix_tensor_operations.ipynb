{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Matrices and Tensors in PyTorch\n",
    "Date: 2019-08-26  \n",
    "Author: Jason Beach  \n",
    "Categories: Introduction_Tutorial, Data_Science  \n",
    "Tags: numpy, pytorch, python, deeplearning  \n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Artificial Intelligence field is moving from single-function libraries to frameworks for building different network models.  TensorFlow was one of the first, and has strong production capabilities, such as process optimizations.  However, its syntax is unintuitive, and the library has a reputation for being difficult for testing new models.  This led to many organizations adopting PyTorch, with underlying Numpy, for designing network models.  This post describes the basic data structures for working with Matrices and Tensors in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Tensors and Matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic terminology\n",
    "\n",
    "Before we begin performing opertions, we need to have a shared vocabulary on the of the three main data structures using in Artificial Intelligence (AI).\n",
    "\n",
    "* scalar is a single number\n",
    "* vector is just one row or column\n",
    "* matrix is just a 2-D grid of numbers\n",
    "* tensor is a 'placeholder' for the a multi-dimensional array (vector, matrix, etc.)\n",
    "\n",
    "We should discuss tensor in more detail because a 'placeholder' is not a very mathematical definition, and it is often confused with a matrix. A tensor is often thought of as a generalized matrix. That is, it could be a 1-D matrix (a vector is actually such a tensor), a 3-D matrix (something like a cube of numbers), even a 0-D matrix (a single number), or a higher dimensional structure that is harder to visualize. The dimension of the tensor is called its rank.\n",
    "\n",
    "A tensor is a mathematical entity that lives in a structure and interacts with other mathematical entities. If one transforms the other entities in the structure in a regular way, then the tensor must obey a related transformation rule.\n",
    "\n",
    "This ‚Äúdynamical‚Äù property of a tensor is the key that distinguishes it from a mere matrix. It‚Äôs a team player whose numerical values shift around along with those of its teammates when a transformation is introduced that affects all of them.  \n",
    "\n",
    "Any rank-2 tensor can be represented as a matrix, but not every matrix is really a rank-2 tensor. The numerical values of a tensor‚Äôs matrix representation depend on what transformation rules have been applied to the entire system.\n",
    "\n",
    "Indeed, speaking of a rank-2 tensor is not really accurate. The rank of a tensor has to be given by two numbers. The vector to vector mapping is given by a rank-(1,1) tensor, while the quadratic form is given by a rank-(0,2) tensor. There's also the type (2,0) which also corresponds to a matrix, but which maps two covectors to a number, and which again transforms differently.\n",
    "\n",
    "So, succinctly, a tensor is different from a matrix in that a tensor can be:\n",
    "\n",
    "* rank (dimension) greater than 2\n",
    "* part of a system, such as a neural network, that accounts for inter-related entities\n",
    "* dynamic, such that, just as a random variable X can have an actualized value, x, a tensor can be a container for actualized values of an array \n",
    "\n",
    "The bottom line of this is:\n",
    "\n",
    "* components of a rank-2 tensor can be written in a matrix.\n",
    "* the tensor is not that matrix, because different types of tensors can correspond to the same matrix.\n",
    "* the differences between those tensor types are uncovered by the basis transformations (hence the physicist's definition: \"A tensor is what transforms like a tensor\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The need for the Tensor construct\n",
    "\n",
    "You can specify a linear transformation ùëé between vectors by a matrix. Let's call that matrix ùê¥. Now if you do a basis transformation, this can also be written as a linear transformation, so that if the vector in the old basis is ùë£, the vector in the new basis is $T^{-1}ùë£$ (where v is a column vector). Now you can ask what matrix describes the transformation ùëé in the new basis. Well, it's the matrix $T^{-1}AT$.\n",
    "\n",
    "Well, so far, so good. What I memorized back then is that under basis change a matrix transforms as $T^{-1}AT$.\n",
    "\n",
    "But then, we learned about quadratic forms. Those are calculated using a matrix ùê¥ as $u^TAv$. Still no problem, until we learned about how to do basis changes. Now, suddenly the matrix did not transform as $T^{-1}AT$, but rather as $T^TAT$. Which confused me like hell: How could one and the same object transform differently when used in different contexts?\n",
    "\n",
    "Well, the solution is: Because we are actually talking about different objects! In the first case, we are talking about a tensor which takes vectors to vectors. In the second case, we are talking about a tensor which takes two vectors into a scalar, or equivalently, which takes a vector to a covector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof basis change rule for quadratic form, with ùëÉ being the change of basis matrix between ùë•,ùë¶,ùëß and ùë¢,ùë£,ùë§\n",
    "\n",
    "\\begin{align}q&= \\begin{bmatrix}x&y&z\\end{bmatrix}\\begin{bmatrix}\\text A\\end{bmatrix}\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}\\\\ &=\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}^\\top\\begin{bmatrix}\\text A\\end{bmatrix}\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}\\\\ &=\\left(P\\begin{bmatrix}u\\\\v\\\\w\\end{bmatrix}\\right)^\\top \\begin{bmatrix}\\text A\\end{bmatrix}P\\begin{bmatrix}u\\\\v\\\\w\\end{bmatrix}\\\\ &=\\begin{bmatrix}u&v&w\\end{bmatrix}P^\\top\\begin{bmatrix}\\text A\\end{bmatrix}P\\begin{bmatrix}u\\\\v\\\\w\\end{bmatrix} \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our discussion of tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy ndarray\n",
    "\n",
    "PyTorch is 'A replacement for Numpy to use the power of GPUs.'  PyTorch provides both matrices and tensors.  Let's dive into Numpy to see what PyTorch is replacing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the core, numpy provides the excellent ndarray objects, short for n-dimensional arrays.\n",
    "\n",
    "In a ‚Äòndarray‚Äô object, aka ‚Äòarray‚Äô, you can store multiple items of the same data type. It is the facilities around the array object that makes numpy so convenient for performing math and data manipulations.\n",
    "\n",
    "* arrays are designed to handle vectorized operations while a python list is not\n",
    "* array size cannot be increased (unlike list), so a new array must be created\n",
    "* array memory size is much smaller than list\n",
    "* an array must have all items be of the same data type, unlike lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lack of vectorization makes this impossible\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [0,1,2,3,4]\n",
    "arr1d = np.array(list1)\n",
    "\n",
    "try:\n",
    "    list1 + 2\n",
    "except:\n",
    "    print(\"lack of vectorization on base lists makes this operation impossible\")\n",
    "    \n",
    "print( arr1d + 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly used numpy dtypes are: \n",
    "\n",
    "* `float`\n",
    "* `int`\n",
    "* `bool`\n",
    "* `str`\n",
    "* `object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [3., 4., 5.],\n",
       "       [6., 7., 8.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 2d array from a list of lists\n",
    "list2 = [[0,1,2], [3,4,5], [6,7,8]]\n",
    "arr2d = np.array(list2, dtype='float')\n",
    "arr2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0.0', '1.0', '2.0'],\n",
       "       ['3.0', '4.0', '5.0'],\n",
       "       ['6.0', '7.0', '8.0']], dtype='<U32')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2d.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2d.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (3, 3)\n",
      "Datatype:  float64\n",
      "Size:  9\n",
      "Num Dimensions:  2\n"
     ]
    }
   ],
   "source": [
    "# memory address\n",
    "print('Memory: ', arr2d.data)\n",
    "# shape\n",
    "print('Shape: ', arr2d.shape)\n",
    "# dtype\n",
    "print('Datatype: ', arr2d.dtype)\n",
    "# size\n",
    "print('Size: ', arr2d.size)\n",
    "# ndim\n",
    "print('Num Dimensions: ', arr2d.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3., -1., -1.,  6.,  7.,  8.])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(arr2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  3.,  6.],\n",
       "       [ 1., -1.,  7.],\n",
       "       [ 2., -1.,  8.]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transpose\n",
    "arr2d.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04166667,  0.25      ,  0.04166667],\n",
       "       [-1.25      , -0.5       ,  0.25      ],\n",
       "       [ 1.125     ,  0.25      , -0.125     ]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#matrix inversion\n",
    "linalg.inv(arr2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm\n",
    "linalg.norm(arr2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.000000000000004"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#determinent\n",
    "linalg.det(arr2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slicing\n",
    "arr2d[:2,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False],\n",
       "       [False, False,  True],\n",
       "       [ True,  True,  True]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#boolean indexing\n",
    "arr2d > 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6., 7., 8.],\n",
       "       [3., 4., 5.],\n",
       "       [0., 1., 2.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reverse the array on one dimen (rows)\n",
    "arr2d[::-1,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.]\n",
      " [ 3. nan inf]\n",
      " [ 6.  7.  8.]]\n",
      "\n",
      "[[ 0.  1.  2.]\n",
      " [ 3. -1. -1.]\n",
      " [ 6.  7.  8.]]\n",
      "\n",
      "[[0. 1. 2.]\n",
      " [3. 4. 5.]\n",
      " [6. 7. 8.]]\n"
     ]
    }
   ],
   "source": [
    "#deep copy of array\n",
    "tmp = arr2d.copy()\n",
    "\n",
    "#NaN or Infinity\n",
    "arr2d[1,1] = np.nan  # not a number\n",
    "arr2d[1,2] = np.inf  # infinite\n",
    "print( arr2d )\n",
    "print()\n",
    "\n",
    "# Replace nan and inf with -1. Don't use arr2 == np.nan\n",
    "missing_bool = np.isnan(arr2d) | np.isinf(arr2d)\n",
    "arr2d[missing_bool] = -1  \n",
    "print( arr2d )\n",
    "print()\n",
    "\n",
    "print( tmp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3. -1. -1.  6.  7.  8.]]\n",
      "[ 0.  1.  2.  3. -1. -1.  6.  7.  8.]\n",
      "[ 0.  1.  2.  3. -1. -1.  6.  7.  8.]\n"
     ]
    }
   ],
   "source": [
    "#reshape\n",
    "print( (arr2d.reshape(9, 1)).transpose())\n",
    "\n",
    "#flatten\n",
    "print( arr2d.flatten())\n",
    "\n",
    "#ravel - new array is reference to the parent array, no copy()\n",
    "print( arr2d.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 2 4 6 8]\n",
      "[10  9  8  7  6  5  4  3  2  1]\n",
      "[ 1  6 11 17 22 28 33 39 44 50]\n"
     ]
    }
   ],
   "source": [
    "# Lower limit is 0 by default\n",
    "print(np.arange(5))  \n",
    "\n",
    "# 0 to 9\n",
    "print(np.arange(0, 10))  \n",
    "\n",
    "# 0 to 9 with step of 2\n",
    "print(np.arange(0, 10, 2))  \n",
    "\n",
    "# 10 to 1, decreasing order\n",
    "print(np.arange(10, 0, -1))\n",
    "\n",
    "# Exactly 10 numbers, Start at 1 and end at 50 (not equally spaced because of the int rounding)\n",
    "print( np.linspace(start=1, stop=50, num=10, dtype=int))\n",
    "\n",
    "\n",
    "# LogSpace - start value is actually base^start and ends with base^stop (default base 10)\n",
    "# Limit the number of digits after the decimal to 2\n",
    "np.set_printoptions(precision=2)  \n",
    "# Start at 10^1 and end at 10^50\n",
    "print( np.logspace(start=1, stop=50, num=10, base=10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#zeros\n",
    "print( np.zeros([2,2]))\n",
    "\n",
    "#ones\n",
    "print( np.ones([2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile:    [1 2 3 1 2 3]\n",
      "Repeat:  [1 1 2 2 3 3]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3] \n",
    "# Repeat whole of 'a' two times\n",
    "print('Tile:   ', np.tile(a, 2))\n",
    "\n",
    "# Repeat each element of 'a' two times\n",
    "print('Repeat: ', np.repeat(a, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 8 3 7 7 0 4 2 5 2]\n",
      "Unique items :  [0 2 3 4 5 7 8]\n",
      "Counts       :  [1 2 1 1 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Create random integers of size 10 between [0,10)\n",
    "np.random.seed(100)\n",
    "arr_rand = np.random.randint(0, 10, size=10)\n",
    "print(arr_rand)\n",
    "\n",
    "# Get the unique items and their counts\n",
    "uniqs, counts = np.unique(arr_rand, return_counts=True)\n",
    "print(\"Unique items : \", uniqs)\n",
    "print(\"Counts       : \", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good understanding how to work with Numpy, lets see how the PyTorch Tensor may be a 'replacement' as it states in its purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's forget our original discussion of a Tensor, for a moment, and focus on the concrete PyTorch implementation.  A `torch.Tensor` is a multi-dimensional matrix containing elements of a single data type.  When an uninitialized matrix is created, whatever values were in the allocated memory at the time will appear as the initial values.  `torch.Tensor` is an alias for the default tensor type, `torch.FloatTensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation\n",
    "\n",
    "Since the `torch.Tensor` attempts to be a replacement for `ndarray`, we expect to see a similar class API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(5, 3)    #construct a 5x3 matrix, uninitialized\n",
    "x = torch.rand(5, 3)     #randomly initialized matrix\n",
    "x = torch.zeros(5, 3, dtype=torch.long)    #matrix filled zeros and of dtype long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.LongTensor\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print( x.type() )\n",
    "\n",
    "print( x.shape )\n",
    "print( x.size() )\n",
    "print( x.dim() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can be constructed from a Python list or sequence using the torch.tensor() constructor.  \n",
    "\n",
    "`torch.tensor()` always copies data. If you have a Tensor data and just want to change its `requires_grad` flag, use `requires_grad_()` or `detach()` to avoid a copy. If you have a numpy array and want to avoid a copy, use `torch.as_tensor()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.5, 3])    #create a tensor from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor\n",
    "new_tensor = torch.Tensor([[1, 2], [3, 4]])    # create a 2 x 3 tensor with random values\n",
    "empty_tensor = torch.Tensor(2, 3)    # create a 2 x 3 tensor with random values between -1and 1\n",
    "uniform_tensor = torch.Tensor(2, 3).uniform_(-1, 1)    # create a 2 x 3 tensor with random values from a uniform distribution on the interval [0, 1)\n",
    "rand_tensor = torch.rand(2, 3)    # create a 2 x 3 tensor of zeros\n",
    "zero_tensor = torch.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection and reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## slicing examples\n",
    "slice_tensor = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])    # elements from every row, first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "print( slice_tensor[1][0] )\n",
    "print( slice_tensor[1][0].item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 4., 7.])\n",
      "tensor([3., 6., 9.])\n",
      "tensor([7., 8., 9.])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(slice_tensor[:, 0])         # tensor([ 1.,  4.,  7.])# elements from every row, last column\n",
    "print(slice_tensor[:, -1])        # tensor([ 3.,  6.,  9.])# all elements on the second row\n",
    "print(slice_tensor[2, :])         # tensor([ 4.,  5.,  6.])# all elements from first two rows\n",
    "print(slice_tensor[:2, :])        # tensor([[ 1.,  2.,  3.],\n",
    "                                  #         [ 4.,  5.,  6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4.]])\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.]])\n"
     ]
    }
   ],
   "source": [
    "reshape_tensor = torch.Tensor([[1, 2], [3, 4]])\n",
    "\n",
    "print( reshape_tensor.view(1,4) ) \n",
    "print( reshape_tensor.view(4,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7329, 0.2473, 0.8068],\n",
      "        [0.5935, 0.4460, 0.9787]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(2,3)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple syntaxes for operations.  Any operation that mutates a tensor in-place is post-fixed with an underscore, `_`. For example: `x.copy_(y)`, `x.t_()`, will change x.  While this is a convenient syntax, as opposed to Pandas `inplace=True` argument, take care to shun this mutability if you prefer the functional programming style.\n",
    "\n",
    "The documentation for all operations is [here](https://pytorch.org/docs/stable/torch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-0.2641,  0.1227,  0.8318],\n",
      "        [-0.8145, -0.3912,  0.3452],\n",
      "        [-0.9334, -1.3681,  0.3499],\n",
      "        [-1.9163, -0.0799, -0.7764],\n",
      "        [ 0.7414,  1.4728,  0.0776]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
    "print(x)\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2509,  0.9078,  0.9634],\n",
      "        [-0.5009,  0.2014,  0.7240],\n",
      "        [-0.5479, -0.4745,  0.5204],\n",
      "        [-1.4827,  0.4353, -0.4762],\n",
      "        [ 1.0552,  1.4836,  0.3999]])\n",
      "tensor([[ 0.2509,  0.9078,  0.9634],\n",
      "        [-0.5009,  0.2014,  0.7240],\n",
      "        [-0.5479, -0.4745,  0.5204],\n",
      "        [-1.4827,  0.4353, -0.4762],\n",
      "        [ 1.0552,  1.4836,  0.3999]])\n",
      "tensor([[ 0.2509,  0.9078,  0.9634],\n",
      "        [-0.5009,  0.2014,  0.7240],\n",
      "        [-0.5479, -0.4745,  0.5204],\n",
      "        [-1.4827,  0.4353, -0.4762],\n",
      "        [ 1.0552,  1.4836,  0.3999]])\n"
     ]
    }
   ],
   "source": [
    "#method\n",
    "print(torch.add(x, y))\n",
    "\n",
    "#output tensor as argument\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "\n",
    "#Addition: in-place\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7329, 0.5935],\n",
      "        [0.2473, 0.4460],\n",
      "        [0.8068, 0.9787]])\n",
      "tensor([[0.7329, 0.5935],\n",
      "        [0.2473, 0.4460],\n",
      "        [0.8068, 0.9787]])\n"
     ]
    }
   ],
   "source": [
    "print( my_tensor.t() )             # regular transpose function\n",
    "print( my_tensor.permute(-1,0) )   # transpose via permute function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.3144e-09,  8.3869e-09,  3.0715e-09],\n",
      "        [ 4.7199e-09,  6.7144e-09, -7.1207e-09]])\n"
     ]
    }
   ],
   "source": [
    "cross_prod = my_tensor.cross(my_tensor)\n",
    "print( cross_prod )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2493, 1.3350],\n",
      "        [1.3350, 1.5091]])\n"
     ]
    }
   ],
   "source": [
    "maxtrix_prod = my_tensor.mm( my_tensor.t() )\n",
    "print( maxtrix_prod )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5371, 0.0612, 0.6510],\n",
      "        [0.3523, 0.1989, 0.9579]])\n"
     ]
    }
   ],
   "source": [
    "element_mult = my_tensor.mul(my_tensor)\n",
    "print( element_mult )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on the GPU\n",
    "\n",
    "The crux of the `torch.Tensor` is application to the GPU for faster use in network-based modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_tensor.cuda(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Create a numpy array\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# Convert the numpy array to a torch tensor\n",
    "y = torch.from_numpy(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the torch tensor to a numpy array\n",
    "z = y.numpy()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This post explains the basic operations of the `ndarray` and how `Tensor` adds value.  Later posts will explore how the `Tensor` is used in PyTorch and network modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References: framemworks\n",
    "\n",
    "__Numpy__\n",
    "\n",
    "* [ref: datacamp numpy cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)\n",
    "* [docs: numpy quickstart](https://docs.scipy.org/doc/numpy/user/quickstart.html)\n",
    "* [docs: numpy api](https://docs.scipy.org/doc/numpy/reference/)\n",
    "\n",
    "\n",
    "__PyTorch__\n",
    "\n",
    "* [docs: pytorch cheatsheet](https://pytorch.org/tutorials/beginner/ptcheat.html)\n",
    "* [docs: pytorch api](https://pytorch.org/docs/stable/index.html)\n",
    "* [ref: kdnuggets pytorch](https://www.kdnuggets.com/2019/08/pytorch-cheat-sheet-beginners.html)\n",
    "* [ref: pytorch cheat](https://github.com/Tgaaly/pytorch-cheatsheet/blob/master/README.md)\n",
    "* [ref: kaggle pytorch starter with helper functions](https://github.com/bfortuner/pytorch-kaggle-starter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
