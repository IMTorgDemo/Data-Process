{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Refresher Tutorial\n",
    "Date: 2020-06-15  \n",
    "Author: Jason Beach  \n",
    "Categories: Introduction_Tutorial, Data_Science  \n",
    "Tags: pyspark, python, spark  \n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is the primier BigData tool for data science, and PySpark supports a natural move from the local machine to cluster computing.  In fact, you can use PySpark on your local machine in standalone mode just as you would on a cluster.  In this post, we provide a refresher for those working on legacy or other systems, and want to quickly transition back to Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "When using the pyspark-shell, the `spark.sparkContext` is available via `sc` environment variable.  However, in Jupyter, we will have to create our own session.  Take note of the version, Spark v3.0 had some big changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "          .appName(\"app.name\") \\\n",
    "          .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7ffeadb3f712:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>app.name</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=app.name>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Always get a sense of the raw data.  Typically, this can only be done via commandline.  Here, we see there is some metadata as a header, before the actual data.  This would cause some problems if we were loading the data with a simple dataframe command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S&P Letters Data\n",
      "We collected information on the variables using all the block groups in California from the 1990 Census. In this sample a block group on average includes 1425.5 individuals living in a geographically compact area. Naturally, the geographical area included varies inversely with the population density. We computed distances among the centroids of each block group as measured in latitude and longitude. We excluded all the block groups reporting zero entries for the independent and dependent variables. The final data contained 20,640 observations on 9 variables. The dependent variable is ln(median house value).\n",
      "\n",
      "\t\tBols\ttols\n",
      "INTERCEPT\t\t11.4939\t275.7518\n",
      "MEDIAN INCOME\t\t0.4790\t45.7768\n",
      "MEDIAN INCOME2\t\t-0.0166\t-9.4841\n",
      "MEDIAN INCOME3\t\t-0.0002\t-1.9157\n",
      "ln(MEDIAN AGE)\t\t0.1570\t33.6123\n",
      "ln(TOTAL ROOMS/ POPULATION)\t-0.8582\t-56.1280\n",
      "ln(BEDROOMS/ POPULATION)\t0.8043\t38.0685\n",
      "ln(POPULATION/ HOUSEHOLDS)\t-0.4077\t-20.8762\n",
      "ln(HOUSEHOLDS)\t\t0.0477\t13.0792\n",
      "\n",
      "The file cadata.txt contains all the the variables. Specifically, it contains median house value, median income, housing median age, total rooms, total bedrooms, population, households, latitude, and longitude in that order. \n"
     ]
    }
   ],
   "source": [
    "! head -n 15 ./Data/spark_Houses/cadata.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets\n",
    "\n",
    "The Resilient Distributed Dataset (RDD) is a really fun way for tackling string data.  It has strong support for functional programming, so you can get initial processing completed in a very clean manner.\n",
    "\n",
    "Its important to note that if you perform operations that require shuffling data among the different Tasks, that your Job will be slowed, considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"./Data/spark_Houses/cadata.txt\")    #\"hdfs://<HDFS loc>/data/*.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = textFile.take(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is incorrect, but a fun exercise\n",
    "#also possible to use .lookup(1), in-place of filter(lambda x: x[0]>4)\n",
    "headers = textFile.zipWithIndex().\\\n",
    "    map(lambda x: (x[1],x[0]) ).\\\n",
    "    filter(lambda x: x[0]>4).\\\n",
    "    filter(lambda x: x[0]<13).\\\n",
    "    map(lambda x: (x[1].split(\"\\t\"))[0] ).\\\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housingMedianAge',\n",
       " 'totalRooms',\n",
       " 'totalBedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'medianIncome',\n",
       " 'medianHouseValue']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = [u'longitude: continuous.', u'latitude: continuous.', u'housingMedianAge: continuous. ', u'totalRooms: continuous. ', u'totalBedrooms: continuous. ', u'population: continuous. ', u'households: continuous. ', u'medianIncome: continuous. ', u'medianHouseValue: continuous. ']\n",
    "header = [x.split(\": \")[0] for x in col_names]\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = textFile.zipWithIndex().filter(lambda x: x[1]>27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = rdd.sample(0, .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '4.8900000000000000e+004',\n",
       " '1.7197000000000000e+000',\n",
       " '3.4000000000000000e+001',\n",
       " '1.3790000000000000e+003',\n",
       " '3.3300000000000000e+002',\n",
       " '1.1560000000000000e+003',\n",
       " '3.1500000000000000e+002',\n",
       " '3.5369999999999997e+001',\n",
       " '-1.1897000000000000e+002']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "ptrn = re.compile(\"\\s+\")\n",
    "line = (ln.take(1)[0])[0]\n",
    "re.split(ptrn, line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is prepared, use the Row class to get it into a schema and format that will make import to Spark clean and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT fix dtypes before conversion to DF: \n",
    "#   TypeError: not supported type: <class 'numpy.float64'>\n",
    "rows = rdd.map(lambda x: re.split(ptrn, x[0])).map(lambda x:\n",
    "                                    Row(longitude= np.float64(x[1]),\n",
    "                                        latitude= np.float64(x[2]),\n",
    "                                        housingMedianAge=x[3],\n",
    "                                        totalRooms=x[4],\n",
    "                                        totalBedRooms=x[5],\n",
    "                                        population=x[6],\n",
    "                                        households=x[7],\n",
    "                                        medianIncome=x[8],\n",
    "                                        medianHouseValue=x[9]\n",
    "                                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = rdd.map(lambda x: re.split(ptrn, x[0])).map(lambda x:\n",
    "                                    Row(longitude= x[1],\n",
    "                                        latitude= x[2],\n",
    "                                        housingMedianAge=x[3],\n",
    "                                        totalRooms=x[4],\n",
    "                                        totalBedRooms=x[5],\n",
    "                                        population=x[6],\n",
    "                                        households=x[7],\n",
    "                                        medianIncome=x[8],\n",
    "                                        medianHouseValue=x[9]\n",
    "                                    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe\n",
    "\n",
    "The Spark DataFrame is the workhorse tool for data scientists.  Operations on DataFrames are optimized, so it is better to provide simple instructions to it, than it is to create your own, in say, a User Defined Function (UDF), or some other manner.\n",
    "\n",
    "Ensure your types are correct - if you're coming from a RDD, then they will all be strings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = rows.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('households', 'string'),\n",
       " ('housingMedianAge', 'string'),\n",
       " ('latitude', 'string'),\n",
       " ('longitude', 'string'),\n",
       " ('medianHouseValue', 'string'),\n",
       " ('medianIncome', 'string'),\n",
       " ('population', 'string'),\n",
       " ('totalBedRooms', 'string'),\n",
       " ('totalRooms', 'string')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "df = df_raw.withColumn(\"longitude\", df_raw[\"longitude\"].cast(FloatType()) ) \\\n",
    "   .withColumn(\"latitude\", df_raw[\"latitude\"].cast(FloatType()) ) \\\n",
    "   .withColumn(\"housingMedianAge\", df_raw[\"housingMedianAge\"].cast(FloatType()) ) \\\n",
    "   .withColumn(\"totalRooms\", df_raw[\"totalRooms\"].cast(FloatType()) ) \\\n",
    "   .withColumn(\"totalBedRooms\", df_raw[\"totalBedRooms\"].cast(FloatType()) ) \\\n",
    "   .withColumn(\"population\", df_raw[\"population\"].cast(FloatType()) ) \\\n",
    "   .withColumn(\"households\", df_raw[\"households\"].cast(FloatType()) ) \\\n",
    "   .withColumn(\"medianIncome\", df_raw[\"medianIncome\"].cast(FloatType()) ) \\\n",
    "   .withColumn(\"medianHouseValue\", df_raw[\"medianHouseValue\"].cast(FloatType()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automate column\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names:\n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "columns = df_raw.columns\n",
    "\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "df = convertColumn(df_raw, columns, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('households', 'float'),\n",
       " ('housingMedianAge', 'float'),\n",
       " ('latitude', 'float'),\n",
       " ('longitude', 'float'),\n",
       " ('medianHouseValue', 'float'),\n",
       " ('medianIncome', 'float'),\n",
       " ('population', 'float'),\n",
       " ('totalBedRooms', 'float'),\n",
       " ('totalRooms', 'float')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data starts looking complete, you can begin executing SQL-like commands against the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|housingMedianAge|count|\n",
      "+----------------+-----+\n",
      "|            52.0| 1273|\n",
      "|            51.0|   48|\n",
      "|            50.0|  136|\n",
      "|            49.0|  134|\n",
      "|            48.0|  177|\n",
      "+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"housingMedianAge\").count().sort(\"housingMedianAge\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|   MedianHouseValue|\n",
      "+-------+-------------------+\n",
      "|  count|              20639|\n",
      "|   mean|-119.56957555201876|\n",
      "| stddev|  2.003494699348379|\n",
      "|    min|            -124.35|\n",
      "|    max|            -114.31|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('MedianHouseValue').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Preparing data for models can be more involved than in libraries specific to your local machine.  These are a few of the steps that will need to be completed.  Some of these methods look similar to those of SKLearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add columns\n",
    "\n",
    "We will add a few more columns using the `.withColumn()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Adjust the values of `medianHouseValue`\n",
    "df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|    MedianHouseValue|\n",
      "+-------+--------------------+\n",
      "|  count|               20639|\n",
      "|   mean|-0.00119569575552...|\n",
      "| stddev|2.003494699348537...|\n",
      "|    min|-0.00124349998474...|\n",
      "|    max|-0.00114309997558...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('MedianHouseValue').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(roomsPerHousehold=6.238137082601054, populationPerHousehold=2.109841827768014, bedroomsPerRoom=0.15579659106916466)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Add the new columns to `df`\n",
    "df = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n",
    "   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n",
    "   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "\n",
    "df.select(\"roomsPerHousehold\", \"populationPerHousehold\", \"bedroomsPerRoom\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DenseVector\n",
    "\n",
    "Spark uses [breeze](https://github.com/scalanlp/breeze) under the hood for high performance Linear Algebra in Scala.\n",
    "\n",
    "In Spark MLlib and ML some algorithms depends on `org.apache.spark.mllib.libalg.Vector` type which is rather dense (`DenseVector`) or sparse (`SparseVector`).\n",
    "\n",
    "Their is no implicit conversion between a scala Vector or array into a dense Vector from mllib, so you must ensure this is complete before feeding it to a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep2 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order and select columns\n",
    "df = df.select(\"medianHouseValue\", \n",
    "              \"totalBedRooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"medianIncome\", \n",
    "              \"roomsPerHousehold\", \n",
    "              \"populationPerHousehold\", \n",
    "              \"bedroomsPerRoom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define the `input_data` \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.0012222000122070313,\n",
       "  DenseVector([1106.0, 2401.0, 1138.0, 37.86, 6.2381, 2.1098, 0.1558])),\n",
       " (-0.0012223999786376953,\n",
       "  DenseVector([190.0, 496.0, 177.0, 37.85, 8.2881, 2.8023, 0.1295])),\n",
       " (-0.0012225,\n",
       "  DenseVector([235.0, 558.0, 219.0, 37.85, 5.8174, 2.5479, 0.1845]))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace `df` with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=-0.0012222000122070313, features=DenseVector([1106.0, 2401.0, 1138.0, 37.86, 6.2381, 2.1098, 0.1558])),\n",
       " Row(label=-0.0012223999786376953, features=DenseVector([190.0, 496.0, 177.0, 37.85, 8.2881, 2.8023, 0.1295])),\n",
       " Row(label=-0.0012225, features=DenseVector([235.0, 558.0, 219.0, 37.85, 5.8174, 2.5479, 0.1845]))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize columns\n",
    "\n",
    "The `StandardScaler` is used to transform a column to zero mean and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `StandardScaler` \n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")   #initialize\n",
    "scaler = standardScaler.fit(df)    #fit\n",
    "scaled_df = scaler.transform(df)   #scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 17.7252, 2.5213, 0.2031, 2.6851])),\n",
       " Row(features_scaled=DenseVector([0.451, 0.438, 0.463, 17.7205, 3.3498, 0.2698, 2.2321]))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the result\n",
    "scaled_df.select(\"features_scaled\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Once the data is prepared, we can choose from a number of different models to apply to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predicted = linearModel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.0011958166787652167, -0.001243499984741211),\n",
       " (-0.0011958166787652167, -0.0012430000305175782),\n",
       " (-0.0011958166787652167, -0.0012430000305175782),\n",
       " (-0.0011958166787652167, -0.0012419000244140626),\n",
       " (-0.0011958166787652167, -0.0012416999816894532)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coefficients for the model\n",
    "linearModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0011958166787652167"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intercept for the model\n",
    "linearModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.004643282977487e-05"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the RMSE\n",
    "linearModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3034018309099338e-13"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the R2\n",
    "linearModel.summary.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These are the basic steps taken in every Spark machine learning application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
