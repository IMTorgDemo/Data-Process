{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Natural Language Understanding Commercial Services with Example Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements code from four different commerical NLP services in a typical workflow.  Each script should be run as a stand-alone implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Commercial references__\n",
    "\n",
    "* [kontikilabs: very thorough with accompanying code](https://medium.com/kontikilabs/comparing-machine-learning-ml-services-from-various-cloud-ml-service-providers-63c8a2626cb6)\n",
    "* [Google vs Watson](http://fredrikstenbeck.com/google-natural-language-vs-watson-natural-language-understanding/)\n",
    "* [Watson internals](https://www.quora.com/What-do-AI-ML-and-NLP-researchers-think-of-IBM%E2%80%99s-Watson-Does-it-have-the-potential-to-make-a-huge-impact)\n",
    "* [Google: categories](https://cloud.google.com/natural-language/docs/categories)\n",
    "* [Watson: categories](https://console.bluemix.net/docs/services/natural-language-understanding/categories.html#categories-hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [IBM Watson](https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 \\\n",
    "  import Features, EntitiesOptions, KeywordsOptions\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "#We need to get our API credentials in the code for authentication that we have stored as Environment Variables locally\n",
    "NLP_USER_WATSON = os.environ.get(\"NLP_USER_WATSON\")\n",
    "NLP_PASS_WATSON = os.environ.get(\"NLP_PASS_WATSON\")\n",
    "NLP_VER_WATSON = os.environ.get(\"NLP_VER_WATSON\")\n",
    "\n",
    "\n",
    "#Following line is used to save all the console output into a text file\n",
    "sys.stdout = open('nlp_api_output.txt', 'a')\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "\n",
    "def input_file(text_file_path):\n",
    "    global text\n",
    "    if os.path.isfile(text_file_path):\n",
    "        with open(text_file_path, 'r') as text_file:\n",
    "            text = text_file.read()\n",
    "    else:\n",
    "        print(\"File doesn't exist in the directory!\")\n",
    "\n",
    "\n",
    "def analyze_text():\n",
    "  #Initialize NaturalLanguageUnderstanding function using the API credentials\n",
    "  natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    username = NLP_USER_WATSON,\n",
    "    password = NLP_PASS_WATSON,\n",
    "    version = NLP_VER_WATSON)\n",
    "\n",
    "  response = natural_language_understanding.analyze(\n",
    "    text = text,\n",
    "    features = Features(\n",
    "      entities = EntitiesOptions(\n",
    "        emotion = True,\n",
    "        sentiment = True),\n",
    "      keywords = KeywordsOptions(\n",
    "        emotion = True,\n",
    "        sentiment = True)))\n",
    "\n",
    "  print(json.dumps(response, indent = 2)) #json output after textual analysis\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description = __doc__,\n",
    "        formatter_class = argparse.RawDescriptionHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        'text_file_path',\n",
    "        help = 'The complete file path of the text file you want to analyze.') \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    input_file(args.text_file_path)\n",
    "    analyze_text()\n",
    "\n",
    "\n",
    "end_time = time.monotonic()\n",
    "print(\"Execution_Time:\", timedelta(seconds = end_time - start_time))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Google Cloud Natural Language](https://cloud.google.com/natural-language/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "\n",
    "#We need to get our API credentials in the code for authentication that we have stored as Environment Variables locally.\n",
    "os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "#Following line is used to save all the console outputs in a text file.\n",
    "sys.stdout = open('nlp_api_content_output.txt', 'w')\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "\n",
    "def input_file(text_file_path):\n",
    "    global text\n",
    "    if os.path.isfile(text_file_path):\n",
    "        with open(text_file_path, 'r') as text_file:\n",
    "            text = text_file.read()\n",
    "    else:\n",
    "        print(\"File doesn't exist in the directory!\")\n",
    "\n",
    "\n",
    "def sentiment_text():\n",
    "    \"\"\"Detects sentiment in the text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "    # Instantiates a plain text document.\n",
    "    document = types.Document(\n",
    "        content = text,\n",
    "        type = enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detects sentiment in the document. You can also analyze HTML with:\n",
    "    #   document.type == enums.Document.Type.HTML\n",
    "    sentiment = client.analyze_sentiment(document).document_sentiment\n",
    "\n",
    "    print('Sentiment: {}, {}'.format(sentiment.score, sentiment.magnitude))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def entities_text():\n",
    "    \"\"\"Detects entities in the text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    document = types.Document(\n",
    "        content = text,\n",
    "        type = enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detects entities in the document. You can also analyze HTML with:\n",
    "    #   document.type == enums.Document.Type.HTML\n",
    "    entities = client.analyze_entities(document).entities\n",
    "\n",
    "    # entity types from enums.Entity.Type\n",
    "    entity_type = ('UNKNOWN', 'PERSON', 'LOCATION', 'ORGANIZATION',\n",
    "                   'EVENT', 'WORK_OF_ART', 'CONSUMER_GOOD', 'OTHER')\n",
    "\n",
    "    for entity in entities:\n",
    "        print('=' * 20)\n",
    "        print(u'{:<16}: {}'.format('name', entity.name))\n",
    "        print(u'{:<16}: {}'.format('type', entity_type[entity.type]))\n",
    "        print(u'{:<16}: {}'.format('metadata', entity.metadata))\n",
    "        print(u'{:<16}: {}'.format('salience', entity.salience))\n",
    "        print(u'{:<16}: {}'.format('wikipedia_url',\n",
    "              entity.metadata.get('wikipedia_url', '-')))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def syntax_text():\n",
    "    \"\"\"Detects syntax in the text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    document = types.Document(\n",
    "        content = text,\n",
    "        type = enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detects syntax in the document. You can also analyze HTML with:\n",
    "    #   document.type == enums.Document.Type.HTML\n",
    "    tokens = client.analyze_syntax(document).tokens\n",
    "\n",
    "    # part-of-speech tags from enums.PartOfSpeech.Tag\n",
    "    pos_tag = ('UNKNOWN', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM',\n",
    "               'PRON', 'PRT', 'PUNCT', 'VERB', 'X', 'AFFIX')\n",
    "\n",
    "    for token in tokens:\n",
    "        print(u'{}: {}'.format(pos_tag[token.part_of_speech.tag],\n",
    "                               token.text.content))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def entity_sentiment_text():\n",
    "    \"\"\"Detects entity sentiment in the provided text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    document = types.Document(\n",
    "        content = text.encode('utf-8'),\n",
    "        type = enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detect and send native Python encoding to receive correct word offsets.\n",
    "    encoding = enums.EncodingType.UTF32\n",
    "    if sys.maxunicode == 65535:\n",
    "        encoding = enums.EncodingType.UTF16\n",
    "\n",
    "    result = client.analyze_entity_sentiment(document, encoding)\n",
    "\n",
    "    for entity in result.entities:\n",
    "        print('Mentions: ')\n",
    "        print(u'Name: \"{}\"'.format(entity.name))\n",
    "        for mention in entity.mentions:\n",
    "            print(u'  Begin Offset : {}'.format(mention.text.begin_offset))\n",
    "            print(u'  Content : {}'.format(mention.text.content))\n",
    "            print(u'  Magnitude : {}'.format(mention.sentiment.magnitude))\n",
    "            print(u'  Sentiment : {}'.format(mention.sentiment.score))\n",
    "            print(u'  Type : {}'.format(mention.type))\n",
    "        print(u'Salience: {}'.format(entity.salience))\n",
    "        print(u'Sentiment: {}\\n'.format(entity.sentiment))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def classify_text():\n",
    "    \"\"\"Classifies content categories of the provided text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    document = types.Document(\n",
    "        content = text.encode('utf-8'),\n",
    "        type = enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    categories = client.classify_text(document).categories\n",
    "\n",
    "    for category in categories:\n",
    "        print(u'=' * 20)\n",
    "        print(u'{:<16}: {}'.format('name', category.name))\n",
    "        print(u'{:<16}: {}'.format('confidence', category.confidence))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description = __doc__,\n",
    "        formatter_class = argparse.RawDescriptionHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        'text_file_path',\n",
    "        help = 'The complete file path of the text file you want to analyze.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    input_file(args.text_file_path)\n",
    "    sentiment_text()\n",
    "    entities_text()\n",
    "    syntax_text()\n",
    "    entity_sentiment_text()\n",
    "    classify_text()\n",
    "\n",
    "\n",
    "end_time = time.monotonic()\n",
    "print(\"Execution_Time:\", timedelta(seconds = end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Amazon Comprehend](https://aws.amazon.com/documentation/comprehend/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "#We need to get our API credentials in the code for authentication that we have stored as Environment Variables locally.\n",
    "os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "os.environ.get(\"AWS_REGION\")\n",
    "    \n",
    "\n",
    "#Following line is used to save all the console outputs in a text file.\n",
    "sys.stdout = open('output.txt','a')\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "\n",
    "def input_file(text_file_path):\n",
    "    global text\n",
    "    if os.path.isfile(text_file_path):\n",
    "        with open(text_file_path, 'r') as text_file:\n",
    "            text = text_file.read()\n",
    "    else:\n",
    "        print(\"File doesn't exist in the directory!\")\n",
    "\n",
    "\n",
    "def dominant_language_text():\n",
    "    #Initialize amazon_comprehend client function\n",
    "    client_comprehend = boto3.client(\n",
    "        'comprehend',\n",
    "    )\n",
    "    dominant_language_response = client_comprehend.detect_dominant_language(\n",
    "        Text = text\n",
    "    )\n",
    "    #Print the Dominant Language\n",
    "    print(\"Language:\", sorted(dominant_language_response['Languages'], key = lambda k: k['LanguageCode'])[0]['LanguageCode'])\n",
    "\n",
    "\n",
    "def entities_text():\n",
    "    #Initialize amazon_comprehend client function\n",
    "    client_comprehend = boto3.client(\n",
    "        'comprehend',\n",
    "    )\n",
    "    response_entities = client_comprehend.detect_entities(\n",
    "            Text = text,\n",
    "            LanguageCode = 'en'\n",
    "    )\n",
    "    entities = list(set([obj['Type'] for obj in response_entities['Entities']]))\n",
    "    #Print the Entities\n",
    "    print(\"Entities:\",entities)\n",
    "\n",
    "\n",
    "def key_phrases_text():\n",
    "    #Initialize amazon_comprehend client function\n",
    "    client_comprehend = boto3.client(\n",
    "        'comprehend',\n",
    "    )\n",
    "    response_key_phrases = client_comprehend.detect_key_phrases(\n",
    "        Text = text,\n",
    "        LanguageCode = 'en'\n",
    "    )\n",
    "    key_phrases = list(set([obj['Text'] for obj in response_key_phrases['KeyPhrases']]))\n",
    "    #Print the Key Phrases\n",
    "    print(\"Key Phrases:\", key_phrases)\n",
    "\n",
    "\n",
    "def sentiment_text():\n",
    "    #Initialize amazon_comprehend client function\n",
    "    client_comprehend = boto3.client(\n",
    "        'comprehend',\n",
    "    )\n",
    "    response_sentiment = client_comprehend.detect_sentiment(\n",
    "        Text = text,\n",
    "        LanguageCode = 'en'\n",
    "    )\n",
    "    sentiment = response_sentiment['Sentiment']\n",
    "    #Print the Sentiment\n",
    "    print(\"Sentiment Analysis:\" , sentiment)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description = __doc__,\n",
    "        formatter_class = argparse.RawDescriptionHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        'text_file_path',\n",
    "        help = 'The complete file path of the text file you want to analyze.')\n",
    "    args = parser.parse_args()\n",
    "    input_file(args.text_file_path)\n",
    "    dominant_language_text()\n",
    "    entities_text()\n",
    "    key_phrases_text()\n",
    "    sentiment_text()\n",
    "\n",
    "\n",
    "end_time = time.monotonic()\n",
    "print(\"Execution_Time:\", timedelta(seconds = end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Microsoft Azure Text Analytics](https://azure.microsoft.com/en-us/resources/videos/learn-how-to-create-text-analytics-solutions-with-azure-machine-learning-templates/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport requests\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import argparse\n",
    "\n",
    "\n",
    "#We need to get our API credentials in the code for authentication that we have stored as Environment Variables locally\n",
    "Ocp_Apim_Subscription_Key = os.environ.get(\"KEY_NLP\")\n",
    "\n",
    "\n",
    "#Following line is used to save all the console output into a text file\n",
    "sys.stdout = open('nlp_api_output.txt', 'a')\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "\n",
    "def input_file(text_file_path):\n",
    "    global text\n",
    "    if os.path.isfile(text_file_path):\n",
    "        with open(text_file_path, 'r') as text_file:\n",
    "            text = text_file.read()\n",
    "    else:\n",
    "        print(\"File doesn't exist in the directory!\")\n",
    "\n",
    "\n",
    "def analyze_text():\n",
    "    headers = {\n",
    "        # NOTE: Replace the \"Ocp-Apim-Subscription-Key\" value with a valid subscription key.\n",
    "        'Ocp-Apim-Subscription-Key': Ocp_Apim_Subscription_Key,\n",
    "    }\n",
    "\n",
    "    urls = ['https://eastus2.api.cognitive.microsoft.com/text/analytics/v2.0/languages', 'https://eastus2.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment', 'https://eastus2.api.cognitive.microsoft.com/text/analytics/v2.0/keyPhrases']\n",
    "\n",
    "    documents = { 'documents': [\n",
    "        { 'id': '1', 'language': 'en', 'text': text }]}\n",
    "\n",
    "    try:\n",
    "        # NOTE: You must use the same location in your REST call as you used to obtain your subscription keys.\n",
    "        #   For example, if you obtained your subscription keys from westus, replace \"eastus2\" in the\n",
    "        #   URLs above with \"westus\".\n",
    "        for url in urls:\n",
    "            response = requests.post(url = url,\n",
    "                                 headers = headers,\n",
    "                                 data = (json.dumps(documents)).encode('utf-8'))\n",
    "            data = response.json()\n",
    "            print(data)\n",
    "        print('\\n')\n",
    "    except Exception as e:\n",
    "        print('Error: ', e)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description = __doc__,\n",
    "        formatter_class = argparse.RawDescriptionHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        'text_file_path',\n",
    "        help = 'The complete file path of the text file you want to analyze.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    input_file(args.text_file_path)\n",
    "    analyze_text()\n",
    "\n",
    "\n",
    "end_time = time.monotonic()\n",
    "print(\"Execution_Time:\", timedelta(seconds = end_time - start_time))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF DOCUMENT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
