{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Deployments\n",
    "Date: 2020-03-03  \n",
    "Author: Jason Beach  \n",
    "Categories: BigData, BestPractice  \n",
    "Tags: spark, deployment  \n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is seen as the staple of clusters and distributed management.  Spark is ubiquitous data science tool.  What if you combine Hadoop with Spark?  We will explore that question and compare different deployment architectures in this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As Storage you use HDFS. Analytics is done with Apache Spark and YARN is taking care of the resource management.  Why does that work so well together?\n",
    "\n",
    "From a platform architecture perspective, Hadoop and Spark are usually managed on the same cluster. This means on each server where a HDFS data node is running, a Spark worker thread runs as well.\n",
    "\n",
    "In distributed processing, network transfer between machines is a large bottle neck. Transferring data within a machine reduces this traffic significantly.  Spark is able to determine on which data node the needed data is stored. This allows a direct load of the data from the local storage into the memory of the machine.  This reduces network traffic a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark on Yarn\n",
    "\n",
    "You need to make sure that your physical resources are distributed perfectly between the services. This is especially the case when you run Spark workers with other Hadoop services on the same machine.\n",
    "\n",
    "It just would not make sense to have two resource managers managing the same server's resources. Sooner or later they will get in each others way.\n",
    "\n",
    "That's why the Spark standalone resource manager is seldom used.\n",
    "\n",
    "So, the question is not Spark or Hadoop. The question has to be: Should you use Spark or MapReduce alongside Hadoop's HDFS and YARN.\n",
    "\n",
    "* Spark is an in-memory distributed computing engine.\n",
    "* Hadoop is a framework for distributed storage (HDFS) and distributed processing (YARN).\n",
    "* Spark can run with or without Hadoop components (HDFS/YARN)\n",
    "\n",
    "Since Spark does not have its own distributed storage system, it has to depend on one of these storage systems for distributed computing.\n",
    "\n",
    "* S3 – Non-urgent batch jobs. S3 fits very specific use cases when data locality isn’t critical.\n",
    "* Cassandra – Perfect for streaming data analysis and an overkill for batch jobs.\n",
    "* HDFS – Great fit for batch jobs without compromising on data locality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures\n",
    "\n",
    "We can explore some different architectures that are commonly used.  These are constantly evolving as new IAAS and open source projects become available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional\n",
    "\n",
    "The traditional approach is to keep a cluster of servers running 24x7.  The cluster uses HDFS as data storage, and Hadoop yarn managing the servers.  MapReduce can perform simple ETL, but if advanced processing is needed, then Spark can be employed on top of the cluster.  This is used for traditional batch-processing of large volumes of data when software requirements do not allow for the cluster to be automatically configured.  It is also the most expensive design witch typical arrangements costing ~$20K per month.\n",
    "\n",
    "While expensive, you have access to the entire Hadoop ecosystem - the additional applications are trivial to use once you have the hardware.  You use Apache Kafka to ingest data, and store the it in HDFS. You do the analytics with Apache Spark and as a backend for the display you store data in Apache HBase.  To have a working system you also need YARN for resource management. You also need Zookeeper, a configuration management service to use Kafka and HBase.  Spark for instance, can directly access Kafka to consume messages. It is able to access HDFS for storing or processing stored data.  It also can write into HBase to push analytics results to the front end.\n",
    "\n",
    "The cool thing of such ecosystem is that it is easy to build in new functions.  Want to store data from Kafka directly into HDFS without using Spark?  No problem, there is a project for that. Apache Flume has interfaces for Kafka and HDFS.  It can act as an agent to consume messages from Kafka and store them into HDFS. You even do not have to worry about Flume resource management.\n",
    "\n",
    "A large bank can now process payments and update accounts, daily, with just a small team of engineers available for potential problems.  Analysts can create thorough reports from the data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda \n",
    "\n",
    "Once businesses started demanding that the Traditional architecture provide more data, faster, the batch-processing pipeline was not going to deliver.  So, a second pipeline, speed-processing, was setup next to the Traditional for near real-time performance.  may not be as accurate or complete as the ones eventually produced by the batch layer, but they are available almost immediately after data is received.\n",
    "\n",
    "A large bank now has a current view of customer transactions and accounts, and can make tactical decisions, hourly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMACK Stack\n",
    "\n",
    "This high-throughput, scalable design uses all of the latest Apache tools in a streaming manner for the latest Internet-Of-Things (IOT) data ingestion problems.  SMACK is an acronym for: Spark, Mesos, Akka, Cassandra, and Kafka.  Spark and Mesos are obvious replacements for Hadoop.  Akka is an event-driven, functional paradigm for managing all of this.  Cassandra is a data store, while Kafka is a messaging pipe (like RabbitMQ).\n",
    "\n",
    "A large bank can now provide instant feedback to the customer on products customized for their background and the banks current situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Value\n",
    "\n",
    "The deployment that allows you to be most productive for the lowest cost is batch ETL of temporary clusters.  In this arrangement, a cluster is spun-up, data is processed, then the cluster is brought-down.  \n",
    "AWS popularized this technique with S3, Spark, Mesos all coordinated with Lambda.  Challenges arise when working with custom, legacy software that may make automated cluster creation difficult.\n",
    "\n",
    "A bank can now replace its constantly-running cluster with a batch process that may cost a few hundred dollars a week.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These deployment categories are useful as a guide to the different ways Big Data tools can be used together.  If you have the time and resources, and are feeling adventurous, you can try something new on dedicated machines.  However, for solution architects on a budget, these tried-and-true lessons can be employed fairly quickly and with confidence that problems can be fixed with stable solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
