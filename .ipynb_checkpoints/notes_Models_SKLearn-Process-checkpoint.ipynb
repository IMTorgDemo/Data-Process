{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Machine Learning Process\n",
    "\n",
    "* determine problem and constraints\n",
    "* determine characteristics the problem / scenario dictates on the solution\n",
    "\t- model family\n",
    "\t- acceptable methods of dimensionality reduction and regularization\n",
    "\t- primary / secondary evaluation score (ie. accuracy) and methods (ie. confusion matrix, roc)\n",
    "\t\n",
    "* integration and cleaning\t\n",
    "* exploration\n",
    "* address balance (classification, anova, etc.)\n",
    "* create Train, Validate, Test with split (above)\n",
    "\n",
    "* configure Feature Extraction with feature_union\n",
    "* configure Preprocess and choose model-families with pipeline\n",
    "\n",
    "* use k-folds CV and grid search with Training set on multiple model-families and hyper-parameters\n",
    "* evaluate with afore-mentioned confusion matrix, scoring, classifier-threshold, and tests\n",
    "* select the best model-family / hyper-parameters\n",
    "\n",
    "* apply to Validation set or all of Training set to model-family to parameterize it set final model\n",
    "* use Testing set to evaluate final model characteristics\n",
    "\n",
    "_Note:_ Train, Validate, and Test should be from different (independent) data sets, if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data (both numeric and categorical)\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "factor, resp = make_blobs(n_samples=100, centers=2, n_features=50, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = [\"paris\", \"barcelona\", \"kolkata\", \"new york\"]\n",
    "import random\n",
    "[random.sample(data, 2) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begin process with response encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "y = enc.fit_transform(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = factor\n",
    "y = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature union of numeric data\n",
    "features = []\n",
    "features.append(('pca', PCA()))\t\t\t#<<<-grid\n",
    "features.append(('select_best', SelectKBest(k=6))) #<<<-grid\n",
    "num_feature_eng = FeatureUnion(features)\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "#columns_numeric = (X_train.loc[:, X_train.dtypes == np.float64]).columns\n",
    "columns_numeric = list(range(50))\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "\t('num_feature_eng', num_feature_eng)\n",
    "\t])\n",
    "\n",
    "#columns_categoric = (X_train.loc[:, X_train.dtypes != np.float64]).columns\n",
    "columns_categoric = []\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "\t('select_best', SelectKBest(k=6))\n",
    "\t])\n",
    "\t\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, columns_numeric),\n",
    "        ('cat', categorical_transformer, columns_categoric)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(penalty='l1', solver='liblinear'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'preprocessor__num__num_feature_eng__pca__n_components':[.75, .80, .85, .90, .95],\n",
    "        'classifier__C': np.logspace(-4, 4, 5)\n",
    "       }\n",
    "gridClf = GridSearchCV(clf, grid, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num', Pipeline(memory=None,\n",
       "     steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "       strategy='median', verbo...ty='l1', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'preprocessor__num__num_feature_eng__pca__n_components': [0.75, 0.8, 0.85, 0.9, 0.95], 'classifier__C': array([1.e-04, 1.e-02, 1.e+00, 1.e+02, 1.e+04])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridClf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__C': 0.01, 'preprocessor__num__num_feature_eng__pca__n_components': 0.75}\n",
      "Best parameter (CV score=1.000):\n"
     ]
    }
   ],
   "source": [
    "#print( gridClf.best_estimator_ )\n",
    "print( gridClf.best_params_ )\n",
    "print(\"Best parameter (CV score=%0.3f):\" % gridClf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = gridClf.best_estimator_.named_steps['preprocessor']\n",
    "tmp2 = (tmp1.transformers_[0][1].named_steps['num_feature_eng'])\n",
    "tmp3 = dict(tmp2.transformer_list).get('pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of variance explained by components chosen: 0.766\n",
      "Number of components chosen: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Amount of variance explained by components chosen: %0.3f\" % tmp3.explained_variance_ratio_ )\n",
    "print(\"Number of components chosen: %i\" % tmp3.n_components_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: Test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[12,  0],\n",
       "       [ 0,  8]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_test_pred = gridClf.predict(X_test)\n",
    "print( \"Confusion matrix: Test\")\n",
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      1.00      1.00         8\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
