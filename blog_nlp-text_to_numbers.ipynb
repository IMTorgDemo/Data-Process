{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Text to Numbers for Natural Language Processing\n",
    "Date: 2019-11-20  \n",
    "Author: Jason Beach  \n",
    "Categories: DataScience, DeepLearning  \n",
    "Tags: nlp, spacy, python  \n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be making use of the following modules:\n",
    "\n",
    "* numpy - numerical computation\n",
    "* spacy / nltk - word processing\n",
    "* gensim - model implementation\n",
    "* fasttext - word vectors\n",
    "* keras (tensorflow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_References_\n",
    "\n",
    "* [doc: spacy similarity](https://spacy.io/usage/vectors-similarity)\n",
    "* [ref: skip-gram training with numpy](https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72)\n",
    "* [ref: performance comparison](https://github.com/TomLin/Playground/blob/master/04-Model-Comparison-Word2vec-Doc2vec-TfIdfWeighted.ipynb)\n",
    "* [ref: explore cosing similiarity](http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/)\n",
    "* [ref: comparison of word vectors](https://github.com/TomLin/Playground/blob/master/04-Model-Comparison-Word2vec-Doc2vec-TfIdfWeighted.ipynb)\n",
    "* [app: explore embeddings with graph](https://github.com/anvaka/word2vec-graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Discussion and references from multiple sources in `@@Deep Learning for NLP`\n",
    "\n",
    "Word embedding just represents words with numbers.  Word2vec is a neural network structure to generate word embedding by training the model on a supervised classification problem. Such a method was first introduced in the paper [Efficient Estimation of Word Representations in Vector Space by Mikolov et al.,2013](https://arxiv.org/pdf/1301.3781.pdf) and was proven to be quite successful in achieving word embedding that could used to measure syntactic and semantic similarities between words.\n",
    "\n",
    "Although one-hot encoding is quite simple, there are several downsides. The most notable one is that it is not easy to measure relationships between words in a mathematical way.  Also, the matrix used would have to be huge (~1M columns) to contain a quality number of words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word vector is a table of numbers, with one row per term in your vocabulary.  If two terms are used in similar contexts, the algorithm that learns the vectors should assign them rows that are quite similar.  Word vectors should be discussed when used on the 'word', 'sentence', and 'document' levels. \n",
    "\n",
    "When training models, word vectors are useful in the context of Named Entity Recognition.  Although training data may not have an instance of a specific NE, two similar entities should have similar vectors.  So, training without the full domain of possible NEs should not effect the model too adversely.\n",
    "\n",
    "Word usage has a Pareto distribution where most words are rare, so most of the rows in a large word vectors table will be accessed very rarely, or never at all. You can usually cover more than 95% of the tokens in your corpus with just a few thousand rows in the vector table.  Adding more rows of words has rapidly diminishing returns.\n",
    "\n",
    "Custom word vectors can be trained using a number of open-source libraries, such as Gensim, Fast Text, or Tomas Mikolov’s [original word2vec implementation](https://code.google.com/archive/p/word2vec/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use similarity, you need a larger spaCy model that has word vectors included (`en_core_web_lg`, `en_core_web_md` – but not `_sm`).  That is because `similarity` is determined using word vectors.  Word vectors are generated using an algorithm like Word2Vec and lots of text.  The default distance is `cosine` similarity, but can be adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cosine similarity](images/nlp-wordembed_cosinesim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more in-depth look, the [source code](https://spacy.io/docs/usage/word-vectors-similarities) for `.similarity` shows:\n",
    "\n",
    "`return numpy.dot(self.vector, other.vector) / (self.vector_norm * other.vector_norm)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage in SpaCy\n",
    "\n",
    "Spacy's `en_vectors_web_lg` model provides 300-dimensional GloVe vectors for over 1 million terms of English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download en_core_web_lg --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "tokens = nlp(\"dog cat banana afskfsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banana\n",
      "True\n",
      "False\n",
      "(300,)\n",
      "[ 0.20228  -0.076618  0.37032   0.032845 -0.41957   0.072069 -0.37476\n",
      "  0.05746  -0.012401  0.52949  -0.5238   -0.19771  -0.34147   0.53317\n",
      " -0.025331  0.1738    0.16772   0.83984   0.055107  0.10547 ]\n"
     ]
    }
   ],
   "source": [
    "print( tokens[2] )\n",
    "print( tokens[2].has_vector )\n",
    "print( tokens[2].is_oov )\n",
    "print( tokens[2].vector.shape )\n",
    "print( tokens[2].vector[0:20] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8016855120658875"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "#similarity = 1 - cosine_distance\n",
    "1 - cosine(tokens[0].vector, tokens[1].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within animals:  0.80168545\n",
      "Between animals and fruit:  0.2815437\n"
     ]
    }
   ],
   "source": [
    "print( \"Within animals: \", tokens[0].similarity(tokens[1]) )\n",
    "print( \"Between animals and fruit: \", tokens[1].similarity(tokens[2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax_x = np.array(range(21))\n",
    "plt.bar(ax_x,  tokens[2].vector[0:21])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.79927266\n"
     ]
    }
   ],
   "source": [
    "#document-level\n",
    "# use list comprehension to get the vectors for each word\n",
    "word_vector_list = [tok.vector for tok in tokens]\n",
    "\n",
    "# calculate the mean across each word\n",
    "average_word_vector = np.mean(word_vector_list, axis=0)\n",
    "\n",
    "# check that the sum is the same as the other way\n",
    "print(average_word_vector.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.79927266"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tokens.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dog, cat, banana, afskfsd]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stop words, so that only most important are used\n",
    "strong_words = [tok for tok in tokens if tok.is_stop == False]\n",
    "strong_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce 300 dimensions to just 2 using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.09993372, -2.28173842],\n",
       "       [ 3.59422001, -2.38095955],\n",
       "       [-3.54300949, -0.36220067],\n",
       "       [ 2.59345655,  6.11416944],\n",
       "       [-1.08535571, -1.63163824],\n",
       "       [-2.43593785,  0.95236696],\n",
       "       [-3.22330723, -0.40999952]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the PCA module from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# this is just making sure we have loaded in our word vectors\n",
    "if 'nlp' not in locals():\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def get_word_vectors(words):\n",
    "    # converts a list of words into their word vectors\n",
    "    return [nlp(word).vector for word in words]\n",
    "\n",
    "words = ['car', 'truck', 'dragon', 'data', 'horse', 'fish' , 'lion']\n",
    "\n",
    "# intialise pca model and tell it to project data down onto 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# fit the pca model to our 300D data, this will work out which is the best \n",
    "# way to project the data down that will best maintain the relative distances \n",
    "# between data points. It will store these intructioons on how to transform the data.\n",
    "pca.fit(get_word_vectors(words))\n",
    "\n",
    "# Tell our (fitted) pca model to transform our 300D data down onto 2D using the \n",
    "# instructions it learnt during the fit phase.\n",
    "word_vecs_2d = pca.transform(get_word_vectors(words))\n",
    "\n",
    "# let's look at our new 2D word vectors\n",
    "word_vecs_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasonable locations on the two dimensional scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAAEvCAYAAADSG9NhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAfx0lEQVR4nO3deXRV1d3/8fc3CZBJCGCIkFCiEVAQi5LYVFADyKSg0SrSVRWkzthal8XCUoRardaptX0etLQITmgRI4OUqTL9pFgNTVuZH2sVAkiCEkATJAn790eS230hTN6be2P4vNa6y5x9z9n7eyL5rDPdu805h4iI1IiJdgEiIo2JQlFExKNQFBHxKBRFRDwKRRERj0JRRMQTF41BTz31VJeZmRmNoUWkCVuzZs0u51xqKH1EJRQzMzMpLCyMxtAiJ7W1a9fSo0cPli1bRl5e3nFtM2XKFNq1a0d+fn7DFhcGZvZJqH3o9FlEjmrKlCnMnj072mVEjEJRRMSjUBRpwiZPnkzHjh1JSkpi2LBh7NixI+j9p556ipycHFq1akVaWhrDhg3jww8/DLyfl5fHmjVreOGFFzAzzIzp06cD8OKLL9KnTx/atGlD69at6du3b5O4LBaVa4oi0vDmzJnDmDFjuP3228nPz2fFihWMHj06aJ3i4mLuuusuOnXqxN69e3nuuefo3bs3mzdvplWrVkyePJnvfe97nHHGGUyYMAGArKwsAD7++GNuvPFGsrKyOHDgADNmzODiiy9m7dq1nHHGGRHf37BxzkX81atXLyciDSsnJ8cNHjw4qO3mm292gFu2bNlh61dVVbny8nKXnJzsXnjhhUB7r1693MiRI486VnV1tausrHRdu3Z1P//5z8NR/tcCFLoQ80mnzyJNUHV1NUVFRVx55ZVB7VdffXXQ8rvvvsuAAQNo27YtcXFxJCYm8sUXX7B58+ZjjrFhwwauuuoq0tLSiI2NpVmzZmzatOm4tm3MdPos0gSVlpZSVVVFu3btgtr95S1btjBw4EAuuOACfv/739OhQweaN2/O5Zdfzv79+4/a/759+xg4cCBpaWk8/fTTdOrUifj4eG6++eZjbtvYKRRFmqDU1FTi4uIoKSkJaveXFy5cSHl5OXPmzCEpKQmAqqoqPv/882P2v3r1aoqLi1myZAlnnXVWoH3Pnj1h2oPo0emzSBMxu2gbvR9byunj5nPxEyvI7NKdOXPmBK1TUFAQ+LmiooKYmBji4v57bDRz5kyqqqqCtmnevPlhR38VFRUAtGjRItD217/+lY8//jhcuxM1OlIUaQJmF21jfMEHVFRWA7CtrIKqbsNYOOth7rjjDq666ipWrFjBwoULA9v069eP6upqbrrpJn74wx+ybt06nnzySVJSUoL6Puuss1i0aBGLFi2ibdu2nH766eTm5pKcnMwtt9zCfffdR3FxMZMmTSI9PT2i+90QwnKkaGYpZjbLzDaa2QYz+244+hWR4/PEok2BQKwTl5XL6cN+xLx588jPz6eoqIipU6cG3u/RowfTpk3jb3/7G0OHDmXGjBm8/vrrtGrVKqifBx54gLPPPpvhw4eTk5PDvHnzSEtL4/XXX+fTTz/lyiuv5De/+Q3PPfccZ555ZkT2tyGZC8McLWb2AvD/nHN/NLPmQKJzruxI62dnZ7um8JCnSGNx+rj51PeXbMB/Hrs80uVEjZmtcc5lh9JHyEeKZtYSuBiYCuCcO3C0QBSR8OuQknBC7XJk4Th9PgMoBaaZWZGZ/dHMksLQr4gcp7GDupLQLDaoLaFZLGMHdY1SRd9c4QjFOOB84Fnn3HnAl8C4Q1cys1vNrNDMCktLS8MwrIjUyT8vnUev7kF6SgIGpKck8OjVPcg/75t/4yPSQr6maGanAe865zJrly8CxjnnjnghQ9cURaQhNIpris65T4GtZlZ3nN4fWB9qvyIi0RCu5xR/BLxSe+f5I+CmMPUrIhJRYQlF59w/gJAOWUVEGgN9zE9ExKNQFBHxKBRFRDwKRRERj0JRRMSjUBQR8SgURUQ8CkUREY9CUUTEo1AUEfEoFEVEPApFERGPQlFExKNQFBHxKBRFRDwKRRERj0JRRMSjUBQR8SgURUQ8CkUREY9CUUTEo1AUEfEoFEVEPApFERGPQlFExKNQFBHxKBRFRDwKRRERj0JRRMSjUBQR8SgURUQ8CkUREY9CUUTEo1AUEfEoFEVEPApFERGPQlFExKNQFBHxKBRFRDwKRRERj0JRRMSjUBQR8SgURUQ8CkUREY9CUUTEo1AUEfEoFEVEPApFERGPQlFExBO2UDSzWDMrMrO3wtWniEikhfNI8W5gQxj7ExGJuLCEopllAJcDfwxHfyIi0RKuI8XfAPcBB8PUn4hIVIQcimY2FChxzq05xnq3mlmhmRWWlpaGOqyISIMIx5Fib+AKM/sYeA3oZ2YvH7qSc26Kcy7bOZedmpoahmFFRMIv5FB0zo13zmU45zKBEcBS59z1IVcmIhIFek5RRMQTF87OnHPLgeXh7FNEJJJ0pCgi4lEoioh4FIoiIh6FooiIR6EoIuJRKIqIeBSKIiIehaKIiEehKCLiUSiKiHgUiiIiHoWiiIhHoSgi4lEoioh4FIoiIh6FooiIR6EoIuJRKIqIeBSKIiIehaKIiEehKCLiUSiKiHgUiiIiHoWiiIhHoSgi4lEoioh4FIoiIh6FooiIR6EoIuJRKIqIeBSKIiIehaKIiEehKCLiUSiKiHgUiiIiHoWiiIhHoSgi4lEoioh4FIoiIh6FooiIR6EoIuJRKIqIeBSKIiIehaKIiEehKCLiUSiKiHgUiiIiHoWiiIhHoSgi4gk5FM2so5ktM7MNZrbOzO4OR2EiItEQF4Y+qoB7nXN/N7NTgDVmtsQ5tz4MfYuIRFTIR4rOuR3Oub/X/rwP2ACkh9qviEg0hPWaopllAucBfwtnvyIikRK2UDSzZOAN4CfOub31vH+rmRWaWWFpaWm4hhURCauwhKKZNaMmEF9xzhXUt45zbopzLts5l52amhqOYUVEwi4cd58NmApscM49HXpJIiLRE44jxd7ADUA/M/tH7euyMPQrIhJxIT+S45x7B7Aw1CIiEnX6RIuIiEehKCLiUSiKiHgUiiIiHoWiiIhHoSgi4lEoioh4FIoiIh6FooiIR6EoIuJRKIqIeBSKIiIehaKIiEehKCLiUSiGwUMPPUR6ejoxMTFkZmZiZqxdu/a4tv34448xM956660GrlJEjkc4pjg9qRUWFjJx4kR++ctfkpeXR2JiIhUVFWRlZUW7NBH5GhSKIdq4cSMAY8aMoWXLllGuRkRCpdPnEIwaNYobbrgBgFatWmFmLF++/LDT56lTp9K9e3cSEhI49dRTueSSS1i3bl1QX+Xl5dx22220atWKjIwMJk6cyMGDByO6PyKiUAzJhAkTeOCBBwBYunQpq1evZu/e4NldV65cye23387111/PggULeP7557nwwgvZs2dP0Hr33XcfycnJzJo1i+uvv56HHnqIWbNmRWxfRKSGTp9DkJWVFbh2mJOTQ3JyMsuXLw9a57333uPcc89l/PjxgbYrrrjisL4uvvhinnrqKQAGDBjAwoULKSgoYPjw4Q23AyJyGB0pNrCePXtSVFTEPffcw8qVKzlw4EC96w0cODBouVu3bhQXF0eiRBHxKBQb2KWXXsq0adNYuXIleXl5nHrqqdx55518+eWXQeulpKQELTdv3pz9+/dHslQRQafPX8vsom08sWgT28sqiPv3+mOuP3LkSEaOHElpaSkFBQXcc889tGzZksceeywC1YrIidCR4gmaXbSN8QUfsK2sAgfsLq8EYN4/tx9z29TUVG677TYuuugi1q8/dpiKSOTpSPEEPbFoExWV1Ye1P/OX/+P7vbsc1j5x4kQ+//zzwKlzUVERK1as0FGiSCOlUDxB28sq6m3fsaf+9pycHH7961/z2muvsW/fPjp16sSkSZO4++67G7JMEfmazDkX8UGzs7NdYWFhxMcNh96PLWVbPcGYnpLAqnH9olCRiNQxszXOuexQ+tA1xRM0dlBXEprFBrUlNItl7KCuUapIRMJJp88nKP+8dIDA3ecOKQmMHdQ10C4i32wKxa8h/7x0haBIE6XTZxERj0JRRMSjUBQR8SgURUQ8CkUREY9CUUTEo1AUEfEoFEVEPApFERGPQlFExKNQFBHxKBRFRDwKRRERj0JRRMSjUBQR8SgURUQ8CkUREY9CUUTEo1AUEfGEJRTNbLCZbTKzD81sXDj6FBGJhpBD0cxigf8FhgDdgO+bWbdQ+xURiYZwHCleAHzonPvIOXcAeA24Mgz9iohEXDhCMR3Y6i0X17ZFzNq1azEzli9fHslhRaQJCkcoWj1t7rCVzG41s0IzKywtLQ3DsEc3atQosrOzAZg+fTpmxhdffNHg44rIN1s4QrEY6OgtZwDbD13JOTfFOZftnMtOTU0Nw7DHr1+/fqxevZrExMSIjisi3zzhCMX3gc5mdrqZNQdGAHPD0O8RTZ48mY4dO5KUlMSwYcPYsWNH0Ptmxrp169i6dSupqan069eP3NxcFixYwIABA2jXrh0tW7YkNzeXxYsXH9b/66+/TufOnUlISKBv374UFRVhZkyfPj2wTnV1NZMmTeJb3/oWLVq0oHv37syYMSOon7qj1SVLlnDuueeSlJREnz59WLduXYP8XkQkdCGHonOuCrgLWARsAGY65xrsr37OnDmMGTOGoUOHUlBQQI8ePRg9evRh661du5bKykpeeuklhg0bhpmxceNGhg0bxksvvcTzzz/P3r17GTRoEPHx8eTl5VFYWEhhYSEjRozg/PPPp2XLlgAMHDgQgDFjxjBixAjKysp48MEHeeSRR7j11luZO3cuvXv35gc/+AGvvvpqUB1btmxh7Nix3H///bz66quUlJQwfPhwnDvsCoOINAbOuYi/evXq5b6unJwcN3jw4KC2m2++2QFu2bJlztWkjWvdurWrG2fatGkOcPv27Qts07t3b5eWlua6d+/u+vfv7y666CKXnJzsBg8e7Lp37+4OHjzoOnXq5Dp27OjOOussB7iRI0e6pKQkd9NNN7nExEQ3adKkoDqGDBniunTpElgeOXKki42NdZs3bw60vfnmmw5wGzZs+Nq/AxGpH1DoQsynb9QnWqqrqykqKuLKK4Of+Ln66qsPW7djx46HtW3bto2RI0fStm1bVq1axc6dO1m3bh379+9nwYIFxMfH88477wSOLAGaNWvG66+/DkBeXh4333wzc+bMoby8nGuvvTao/+uuu47NmzdTUlISaMvMzKRz586B5W7dah7hLC4u/pq/BRFpSN+oUCwtLaWqqop27doFtR+6DBAfH39Y2/Dhw/nrX//KRRddREpKCu+//z5Dhgxh//79JCUlMXToUL788kv8G0F9+/alffv2geVu3bqxe/duANLS0oL6r1uuex8gJSUlaJ3mzZsDsH///uPaZxGJrG9UKKamphIXFxd0JAYctnwk//rXv/jd735H+/btycjIIDs7m4qKisD7aWlpxMTE4D8ylJKSErTcvHnzwPXAQ8fduXMnAG3atDmxHRORRqPRh+Lsom30fmwpp4+bz8VPrCCzS3fmzJkTtE5BQcFx99eiRQvat29PSUkJn3zyCatWrQq8t3PnTpKSkpg3b17QjZC5cw+/mZ6QkBA4ra4zc+ZMunTpQqQfORKR8ImLdgFHM7toG+MLPqCishqAbWUVVHUbxsJZD3PHHXdw1VVXsWLFChYuXAjAO/9Xyv3vLgVg/fa9pJ3SPKi/Dh06cO+995Kfn09JSQl9+vQhPb3mwzfl5eXMnz+f/v37M2fOHEaMGEFFRQVr1qwJXP+LiYnh4MGDQM2d6Icffpi4uDiys7MpKCjgz3/+82F3n0Xkm6VRHyk+sWhTIBDrxGXlcvqwHzFv3jzy8/MpKipi6tSpAExZ+R+2ldWcDh+oPsi2sgpmF20LbDtjxgzi4uJ49NFHiY+PZ9++faSnp1NWVsZll11GRUUFjz/+OK+++ipr1qyhpKSEzZs38+yzzwIEHtEBeOCBBxg/fjzPPvssQ4cOZeXKlbz88suMGDGioX8tItKAzD9NjJTs7GxXWFh4zPVOHzf/8M8LUvO5wv88dnlQW+/HlgYCEWDX/F9TuesTsu/+PbectoWbbrqJffv2kZycDNTctLn33nuZN28e+/fv54ILLuDJJ58kJycn0EdmZibXXHMNPXv25IYbbuCjjz5ixYoVh/UlIo2Dma1xzmWH1EdjDsVDg65OekoCq8b1C2o7kQA9ljvuuIMBAwbQunVr/v73v/Pwww/Tu3dv3nrrrRPqR0QiKxyh2KivKY4d1DXomiJAQrNYxg7qeti6HVIS6g3QDikJJzzuZ599xp133slnn31G27Ztue6663j88cdPuB8R+eZp1KGYf17NTZAnFm1ie1kFHVISGDuoa6DddyIBeiwzZ878+kWLyDdaow5FqAnG+kKwvvXg+AJURORIGn0onojjDVARkSNp1I/kiIhEmkJRRMSjUBQR8SgURUQ8CkUREY9CUUTEo1AUEfEoFEVEPApFERGPQlFExKNQFBHxKBRFRDwKRRERj0JRRMSjUBQR8SgURUQ8CkUREY9CUUTEo1AUEfEoFE9Co0aNIjs7pKlxRZoshaKIiEehKGFTWVlJdXX1sVcUacQUiiexJUuWcO6555KUlESfPn1Yt25d4L3y8nJ+/OMfc9pppxEfH09OTg6LFy8O2j4vL49rrrmGKVOmkJWVRXx8PNu3b6e4uJjhw4fTrl07EhISyMrKYsKECUHbvvPOO1xyySUkJibStm1bbrnlFvbt2xeR/RY5miY177Mcvy1btjB27Fjuv/9+EhIS+OlPf8rw4cNZu3YtZsYtt9zC3Llz+eUvf8mZZ57JH/7wBy6//HKWLVtGnz59Av2sWrWKf//73/zqV78iMTGRVq1akZ+fT0VFBVOmTCElJYWPPvqIjRs3Bm3Tv39/8vPzmTVrFp999hnjxo1j9+7dzJo1Kxq/DpH/cs5F/NWrVy8n0TNy5EgXGxvrNm/eHGh78803HeA2bNjg1q9f78zMTZ8+PfB+dXW16969uxs4cGCg7ZJLLnHx8fFux44dQf0nJSW5uXPnHnH8Pn36uLy8vKC2t99+2wHugw8+CHX35CQGFLoQ80mnzyepzMxMOnfuHFju1q0bAMXFxbz//vs457j22msD78fExHDttdfyzjvvBPXTq1cvTjvttKC2nj17Mn78eKZPn86WLVuC3isvL2f16tUMHz6cqqqqwKtPnz40a9aMNWvWhHtXRU6IQvEklZKSErTcvHlzAPbv38+OHTtITk4mMTExaJ20tDTKy8v56quvgtoO9ac//Yns7GzuueceOnXqRM+ePXn77bcB2L17N9XV1dx55500a9Ys8GrRogWVlZVs3bo13LsqckJ0TfEkMLtoG08s2sT2sgo6pCTQ4vPyo67fvn17vvjiC8rLy4OCcefOnSQmJtKiRYtAm5kdtn16ejrTp0/n4MGDvPfee0yaNIkrrriCLVu2kJKSgpkxadIkLrvsssO27dChQwh7KhI6hWITN7toG+MLPqCisuZRmW1lFez+ZDeplZVH3CYnJwczY9asWdx4441AzbXnWbNmBd1kOZaYmBhyc3OZOHEiF154IZ988gnnn38+ubm5bNq0iQcffDC0nRNpAArFJu6JRZsCgVin+qDj0737j7jN2Wefzfe//33uuusu9u7dG7j7vHHjRp599tmjjrdnzx4GDRrEjTfeSJcuXfjqq6946qmnOO200zj77LMBePzxx+nfvz8xMTFcc801nHLKKWzZsoX58+fzyCOP0KVLl9B3XORrUig2cdvLKuptr6w+eNTt/vCHP/Czn/2MX/ziF5SVldGjRw/eeuutYx4pxsfH06NHD5555hm2bt1KYmIiubm5LF68mISEBAD69OnDypUrmThxIjfccAPV1dV06tSJwYMH13uNUiSSrOYudmRlZ2e7wsLCiI97Mur92FK21ROM6SkJrBrXLwoViTQcM1vjnAvpg/26+9zEjR3UlYRmsUFtCc1iGTuoa5QqEmncdPrcxOWflw4QdPd57KCugXYRCaZQPAnkn5euEBQ5Tjp9FhHxKBRFRDwhhaKZPWFmG83sX2b2ppmlHHsrEZHGK9QjxSXAOc65c4HNwPjQSxIRiZ6QQtE5t9g5V1W7+C6QEXpJIiINy8wSjvReOK8pjgYWhLE/EfmGmzlzJtOnTw+5n5UrV9K3b1+Sk5Np1aoVeXl5FBUVsWPHDkaPHs0ZZ5xR94mpc8zsEzN7o25bM8s0M2dmPzCzF82sDJh3pLGO+UiOmf0FOK2et+53zs2pXed+oAp45Sj93ArcCvCtb33rWMOKSBMwc+ZMdu3axahRo752H8uXL2fAgAH07duXF154gaSkJFatWsW2bduIi4ujTZs2PP3007Ru3Zq8vLxPgfbAefV09SRQAFwLHHEyoZA/5mdmI4Hbgf7OuaN/J1UtfcxP5ORwzTXXsGvXLpYvX37EdSorK4mJiSE2Nrbe97/73e9SWVnJ+++/X+9X1fnMbA2QCHQFEpxzB8wsE/gPMNs5d9Wxag717vNg4GfAFccbiCJychg1ahRvvPEGK1aswMwC36N5pAnP6puPfP369bz77rv06tUrEIjV1dU8+uijdOnShRYtWpCSkkJKSkrd6XMv4Gxqsq3ulPSU2v9mm1nqseoO9RMt/wO0AJbUFvyuc+72EPsUkSZgwoQJbNmyhbKyMiZPngxARkYGy5cvr3fCs/rs2bMHgDZt2gTabrvtNl588UXuu+8+du3axZQpU+jatSvPPPMMgwYN2kBNrnUG4s2sDfBy7ab3OedKj1V3SKHonDszlO1FpOnKysqiTZs2HDx4kNzc3KD3ysrKKCoqOmx+n0PVheXnn38OwMaNG5k6dSrPPPMMP/7xj7nwwgsZPnw4r732Wt0m5UBS7c+tqbnPUfeNysc1h64++ywiEVffhGd1/Okz2riaI8WlS5finGPZsmUAgRs3FRUVQdNj1Kr7Us7pwCfAGGD98damUBSRiDvSlwmXlVcGTZ+xs/Yb4j/66D8MGTKENm3aEB8fz9NPP012djYDBgzgt7/9Ld/5znfIysoCyATqnkE8A3gYqP+blo9AoSgiEVffXeT4+Hh27N5Hqjd9RnVFzRlvxqWjKC/fzNKlS6msrOTtt98mPz+fBx98kNLSUh544IG6TRzwIXAO8FtgCjVPxxw3haKIhM2hM0fGflnFgf1Hng/Il5GRQcXnn+KqDmBxtVPufvyPmv+eks7KRX9k06ZNnHXWWVx33XX07NkTgGnTpgX6MLNPgC+AN5xzd5tZBfC/1DwyuPR46lAoikhY1Ddz5Jflp7Dvn4uZPXs2GRkZR53CNj8/nwkPPshnC35LUo9LObDz33zxwV8AaJNUE5Jdu3bl1ltv5d5776WkpISLL76YsrIyZs2a5d9sCXDOjTOzU4A5ZjbAOffusfZDXx0mImFR38yR8d8eQtIZ5zN69GhycnKYMmXKEbc/55xz+NHEpziwfSOlb/yCr7aupe2QuwG4+vz/fq3C5MmTmThxIi+//DKXXXYZP/nJTwKToh3BXcAbwAIz+/ax9kMTV4lIWJw+bj71pYkB/3ns8uPu59BT8BOZPiMcE1fp9FlEwqJDSkK9M0d2SDnqUdxhoj19hk6fRSQsmsrMkTpSFJGwaCozRyoURSRson3qGw46fRYR8SgURUQ8CkUREY9CUUTEo1AUEfEoFEVEPApFERGPQlFExBOVL4Qws1JqviY8Gk4FdkVpbNWgGhprDdA46gi1hk7OuWPO2Hc0UQnFaDKzwlC/RUM1qIamVkNjqaMx1KDTZxERj0JRRMRzMobikb/6N3JUQw3VUKMx1ACNo46o13DSXVMUETmak/FIUUTkiE7KUDSzX5jZv8zsH2a22MyOPMVYw9XwhJltrK3jTTNLiUIN15rZOjM7aGYRveNnZoPNbJOZfWhm4yI5du34z5tZiZmtjfTYXg0dzWyZmW2o/f9wdxRqiDez98zsn7U1/DzSNXi1xJpZkZm9Fa0a4CQNReAJ59y5zrmewFvAg1GoYQlwjnPuXGAzMD4KNawFrgZWRnJQM4ulZi7eIUA34Ptm1i2SNQDTgcERHvNQVcC9zrmzgVxgTBR+D18B/Zxz3wZ6AoPNLDfCNdS5G9gQpbEDTspQdM7t9RaToN5JyBq6hsXOuaraxXeBjKOt30A1bHDObYr0uMAFwIfOuY+ccweA14ArI1mAc24l8Hkkx6ynhh3Oub/X/ryPmkCI6NdWuxpf1C42q31F/O/BzDKAy4E/RnrsQ52UoQhgZo+Y2VbgB0TnSNE3GlgQ5RoiKR3Y6i0XE+EwaGzMLBM4D/hbFMaONbN/ACXAEudcxGsAfgPcBxyMwthBmmwomtlfzGxtPa8rAZxz9zvnOgKvUDNZdsRrqF3nfmpOo16JVg1RYPW0nbSPQZhZMjWTtf/kkLOYiHDOVddeSsoALjCzcyI5vpkNBUqcc2siOe6RNNmJq5xzlx7nqjOA+cDESNdgZiOBoUB/10DPRp3A7yGSioGO3nIGsD1KtUSVmTWjJhBfcc4VRLMW51yZmS2n5lprJG9A9QauMLPLgHigpZm97Jy7PoI1BDTZI8WjMbPO3uIVwMYo1DAY+BlwhXOuPNLjR9n7QGczO93MmgMjgLlRrinizMyAqcAG59zTUaohte7JBzNLAC4lwn8PzrnxzrkM51wmNf8WlkYrEOEkDUXgsdpTyH8BA6m56xVp/wOcAiypfTTouUgXYGZXmVkx8F1gvpktisS4tTeY7gIWUXNzYaZzbl0kxq5jZq8Cq4GuZlZsZj+M5Pi1egM3AP1q/w38o/ZoKZLaA8tq/xbep+aaYlQfiYk2faJFRMRzsh4piojUS6EoIuJRKIqIeBSKIiIehaKIiEehKCLiUSiKiHgUiiIinv8Pu6Bi5mpq/IgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create plot \n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "# plot the scatter plot of where the words will be\n",
    "plt.scatter(word_vecs_2d[:,0], word_vecs_2d[:,1])\n",
    "\n",
    "# for each word and coordinate pair: draw the text on the plot\n",
    "for word, coord in zip(words, word_vecs_2d):\n",
    "    x, y = coord\n",
    "    plt.text(x, y, word, size= 15)\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vector for use with SpaCy\n",
    "\n",
    "Convert the vectors model into a binary format that loads faster and takes up less space on disk.\n",
    "\n",
    "This code outputs a spaCy model in the directory `/tmp/la_vectors_wiki_lg`.  Then load it into spacy.  The model directory will have a /vocab directory with the strings, lexical entries and word vectors from the input vectors model. \n",
    "\n",
    "spaCy’s `Vectors` class lets you map multiple keys to the same row of the table. If you’re using the `spacy init-model` command to create a vocabulary, pruning the vectors will be taken care of automatically if you set the `--prune-vectors` flag.  `Vocab.prune_vectors` reduces the current vector table to a given number of unique entries, and returns a dictionary containing the removed words, mapped to (string, score) tuples.\n",
    "\n",
    "With spacy's `vector` class, data is stored in two structures:\n",
    "\n",
    "* an array, which can be either on CPU or GPU.\n",
    "* a dictionary mapping string-hashes to rows in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.la.300.vec.gz  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy init-model en /tmp/la_vectors_wiki_lg --vectors-loc cc.la.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy init-model en /tmp/la_vectors_wiki_lg --prune-vectors --vectors-loc cc.la.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_latin = spacy.load(\"/tmp/la_vectors_wiki_lg\")\n",
    "doc1 = nlp_latin(\"Caecilius est in horto\")\n",
    "doc2 = nlp_latin(\"servus est in atrio\")\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re using a GPU, it’s much more efficient to keep the word vectors on the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from spacy.vectors import Vectors\n",
    "\n",
    "vector_table = numpy.zeros((3, 300), dtype=\"f\")\n",
    "vectors = Vectors([\"dog\", \"cat\", \"orange\"], vector_table)\n",
    "vectors.data = torch.Tensor(vectors.data).cuda(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627203210548107\n",
      "0.7369546\n"
     ]
    }
   ],
   "source": [
    "# Load a larger model with vectors\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(\"Personal like (2 words in common): \", doc1.similarity(doc2))\n",
    "\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(\"Within food: \", token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc1 = nlp(\"I\")\n",
    "print( doc1.vector.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc1 = nlp(\"I\")\n",
    "print( doc1.vector.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,)   23.1725315055188\n",
      "(96,)   21.75560300132138\n",
      "(96,)   17.23478412191207\n",
      "(96,)   14.848700829346688\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc1 = nlp(\"I\")\n",
    "doc2 = nlp(\"like\")\n",
    "doc3 = nlp(\"I like\")\n",
    "doc4 = nlp(\"I like pizza\")\n",
    "\n",
    "print( doc1.vector.shape, ' ', doc1.vector_norm )\n",
    "print( doc2.vector.shape, ' ', doc2.vector_norm )\n",
    "print( doc3.vector.shape, ' ', doc3.vector_norm )\n",
    "print( doc4.vector.shape, ' ', doc4.vector_norm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1[0].vector == doc3[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.871986\n",
      "4.6767497\n",
      "True\n",
      "4.676749731219555\n"
     ]
    }
   ],
   "source": [
    "print( np.dot(doc1.vector, doc2.vector) )\n",
    "print( np.linalg.norm(doc2.vector, ord=2) )\n",
    "print( np.linalg.norm(doc1.vector, ord=2) == np.linalg.norm(doc2.vector, ord=2) )\n",
    "print( doc2.vector_norm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"pizza like I\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "doc3 = nlp(\"pizza I like\")\n",
    "doc4 = nlp(\"like pizza I\")\n",
    "doc5 = nlp(\"I pizza like\")\n",
    "doc6 = nlp(\"like I pizza\")\n",
    "\n",
    "lDoc = [doc1, doc2, doc3, doc4, doc5, doc6]\n",
    "result = np.zeros((6,6))\n",
    "for i,doc1 in enumerate(lDoc):\n",
    "    for j, doc2 in enumerate(lDoc):\n",
    "        result[i,j] = doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.99999994, 0.99999994, 0.99999995, 0.99999994,\n",
       "        0.99999994],\n",
       "       [0.99999994, 1.        , 0.99999993, 0.99999994, 0.99999993,\n",
       "        0.99999992],\n",
       "       [0.99999994, 0.99999993, 1.        , 0.99999994, 0.99999993,\n",
       "        0.99999993],\n",
       "       [0.99999995, 0.99999994, 0.99999994, 1.        , 0.99999994,\n",
       "        0.99999994],\n",
       "       [0.99999994, 0.99999993, 0.99999993, 0.99999994, 1.        ,\n",
       "        0.99999993],\n",
       "       [0.99999994, 0.99999992, 0.99999993, 0.99999994, 0.99999993,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.895898\n",
      "6.949064\n",
      "7.294794\n",
      "0.77809423\n",
      "0.72417974\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "doc1 = nlp(\"apples oranges fruit\")\n",
    "print( doc1[0].vector_norm )\n",
    "print( doc1[1].vector_norm )\n",
    "print( doc1[2].vector_norm )\n",
    "\n",
    "print( doc1[0].similarity(doc1[1]))\n",
    "print( doc1[0].similarity(doc1[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.895897646384268\n",
      "6.895897762990182\n",
      "1.0000000930092277\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"apples\")\n",
    "doc2 = nlp(\"apples apples apples apples apples apples apples apples apples apples apples apples apples apples apples apples apples apples\")\n",
    "\n",
    "print(doc1.vector_norm)\n",
    "print(doc2.vector_norm)\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.588359567392134\n",
      "6.816139083280562\n",
      "0.9383865534490474\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"apples fruit\")\n",
    "doc2 = nlp(\"apples apples apples apples apples apples apples apples apples apples apples apples apples apples apples apples apples apples fruit\")\n",
    "\n",
    "print(doc1.vector_norm)\n",
    "print(doc2.vector_norm)\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# Load a larger model with vectors\n",
    "#nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "doc = nlp(\"I have a banana\")\n",
    "# Access the word vector via the token.vector attribute\n",
    "vector = doc[3].vector\n",
    "print( type(vector) )\n",
    "print( vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20228 , -0.076618,  0.37032 ,  0.032845, -0.41957 ,  0.072069,\n",
       "       -0.37476 ,  0.05746 , -0.012401,  0.52949 ], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9501447503553421\n"
     ]
    }
   ],
   "source": [
    "#no universal definition for similarity \n",
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Emeddings in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Word Embeddings\n",
    "\n",
    "In Mikolov et al.,2013, two inverse model architectures were presented, 'Continuous Bag-of-Words' model and 'Skip-gram' model.  Skip gram will be discussed here.  Given a 'context word', we want to train a model such that the model can predict a 'target word', one of the words appeared within a predefined window size from the context word.\n",
    "\n",
    "![Skip-gram model](images/nlp-wordembed_skipgram.png)\n",
    "\n",
    "The word embedding layer is essentially a matrix with a shape of (# of unique words in the corpus, word embedding size). Each row of the matrix represent a word in the corpus. Word embedding size is a hyper-parameter to be decided and can be thought as how many features that we would like to use to represent each word. The latter part of the model is simply a logistic regression in a neural network form.\n",
    "\n",
    "In the training process, the word embedding layer and the dense layer are being trained such that the model is able to predict target words given a context word at the end of the training process. After training such a model with a huge amount of data, the word embedding layer will end up becoming a representation of words which could demonstrate many kinds of cool relationships between words in a mathematical way. (Those who are interested in more details can refer to the original paper.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical model-training with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual steps using Numpy\n",
    "\n",
    "* [ref: notebook](https://github.com/ujhuyz0110/wrd_emb/blob/master/word2vec_skipgram_medium_v1.ipynb)\n",
    "* [ref: derive gradient](https://github.com/ujhuyz0110/notes/blob/master/softmax_gradient.pdf)\n",
    "\n",
    "_Generate training data_\n",
    "\n",
    "1. tokenize\n",
    "2. index by assigning integer to each\n",
    "3. create training data\n",
    "\n",
    "_Training process_\n",
    "\n",
    "4. initialize weights (parameters that we want to train)\n",
    "5. propagate forward\n",
    "  - obtain input word’s vector representation from word embedding\n",
    "  - pass the vector to the dense layer\n",
    "  - apply softmax function to the output of the dense layer\n",
    "6. calculate the cost\n",
    "7. propagate backward, and update the weights\n",
    "8. iterate for a number of cycles\n",
    "\n",
    "\n",
    "Tokenize and mapping are here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate training data for the model. For each context word tokens[i], generate: (tokens[i], tokens[i-window_size]), ..., (tokens[i], tokens[i-1]), (tokens[i], tokens[i+1]), ..., (tokens[i], tokens[i+window_size])\n",
    "\n",
    "In the code, the training (x, y) pairs are represented in word ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\"\n",
    "tokens = tokenize(doc)\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two layers in the model needed to be initialized and trained, the word embedding layer and the dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the word embedding will be (vocab_size, emb_size) . Why is that? If we’d like to use a vector with emb_size elements to represent a vocabulary and the total number of vocabularies our corpus is vocab_size, then we can represent all the vocabularies with a vocab_size x emb_size matrix with each row representing a word.\n",
    "\n",
    "The shape of the dense layer will be (vocab_size, emb_size) . How come? The operation that would be performed in this layer is a matrix multiplication. The input of this layer will be (emb_size, # of training instances)and we’d like the output to be (vocab_size, # of training instances)(For each word, we would like to know what the probability that the word appears with the given input word). Note: I do not include biases in the dense layer.\n",
    "\n",
    "Code for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    initialize all the trianing parameters\n",
    "    \"\"\"\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Forward Propagate](images/nlp-wordembed_fwdprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some literatures, the input is presented as a one-hot vector (Let’s say an one-hot vector with i-th element being 1). By multiplying the word embedding matrix and the one-hot vector, we can get the vector representing the input word. However, the result of performing matrix multiplication is essentially the same as selecting the ith row of the word embedding matrix. We can save lots of computational time by simply selecting the row associating with the input word.\n",
    "\n",
    "The rest of the process is just a multi-class linear regression model.\n",
    "The following graph could be used to recall the main operation of the dense layer.\n",
    "\n",
    "Afterwards, we apply softmax function to the output of the dense layer which gives us the probability of each word appearing near the given input word. The following equation could be used to remind what softmax function is.\n",
    "\n",
    "Code for forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we would use cross entropy to calculate cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the back propagation process, we would like to calculate gradients of the trainable weights with respect to the loss function and update the weight with its associated gradient. Back propagation is the methods used to calculate those gradients. It is nothing fancy but chain rule in Calculus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Backward Propagate](images/nlp-wordembed_backwardprop.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    WRD_EMB[inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for many epocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    WRD_EMB[inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of's neighbor words: ['the', 'of', 'costs', 'beating']\n",
      "costs's neighbor words: ['beating', 'of', 'deduction', 'the']\n",
      "is's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "game's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "beating's neighbor words: ['market', 'stock', 'costs', 'investing']\n",
      "a's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "the's neighbor words: ['is', 'of', 'beating', 'stock']\n",
      "loser's's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "market's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "stock's neighbor words: ['investing', 'a', 'is', 'beating']\n",
      "deduction's neighbor words: ['costs', 'after', 'the', 'beating']\n",
      "investing's neighbor words: ['the', 'stock', 'beating', 'costs']\n",
      "after's neighbor words: ['of', 'the', 'deduction', 'costs']\n"
     ]
    }
   ],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]\n",
    "\n",
    "for input_ind in range(vocab_size):\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
