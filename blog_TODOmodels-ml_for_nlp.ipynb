{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Machine Learning Process Applied to Natural Language\n",
    "Date: 2020-28-01  \n",
    "Author: Jason Beach  \n",
    "Categories: Process, DataScience \n",
    "Tags: machine-learning, best-practice, nlp   \n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Machine Learning Process is a reproducible method for developing and deploying automated intelligent solutions.  However, its generality means that it lacks details for specific areas of focus.  A particular area of interest is in solutions for Natural Language Processing (NLP).  NLP encompasses many different problem topics, but we will focus on classification problems, in this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revising the Machine Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit the Machine Learning (ML) Process for NLP problems, we need to make a few revisions.  Let's revisit the diagram to succinctly display the steps.  We will review each step, individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![machine learning process](images/ml_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover\n",
    "\n",
    "We still want to place the primary emphasis on determining the problem and how to create business value.  However, because we are focused on NLP, the pool of model families is reduced.  We also need to think about constraints in time and hardware, as more modern models can be quite demanding.\n",
    "\n",
    "The selection of dimensions will be dependent upon word embeddings and the type of model used.  Newer models, such as FastText, use subsets of characters, instead of whole-word vectors  that are more traditionally used.  For the initial stages of data exploration, more traditional approaches are still valuable.  A few of the ways we can represent language include:\n",
    "\n",
    "* simple occurence count\n",
    "* TF-IDF\n",
    "* trigrams, n-grams\n",
    "* n-charcter subset\n",
    "* word embedding\n",
    "\n",
    "Newer NLP models often have the important aspect of availability of pre-trained models, which can greatly speed-up training and improve primary outcome metrics.  However, pre-trained models can be a hinderance if the text to which the models are applied being appied is different from that which it was originally trained.  Comparing samples of the original text with the target text can be very valuable.  Some methods for doing this are described [here]( {{< ref \"/posts/blog_page-todo.md\" >}}).   \n",
    "\n",
    "A few of the most traditional models, as well as some popular, network-based models include:\n",
    "\n",
    "* Naive bayes\n",
    "* Logistic regression\n",
    "* Support vector machines\n",
    "* FastText\n",
    "* ULMFiT\n",
    "* Googleâ€™s BERT\n",
    "\n",
    "An additional consideration is that customers may have a current system that your solution is meant to replace.  The legacy systems may include lexicons of regular expressions, as well as complicated decision trees.  If the customer expects certain statements to be 'hits' under any condition, then these may have to be incorporated into your solution.  At the least, they will be important aspects of the final evaluation of your solution.\n",
    "\n",
    "Evaluation in text settings is typically based on the Precision-Recall curve.  Perhaps reduction of False Positives is of importance to a compliance office, so Precision should be emphasized.  Maybe every Positive text is important to hit, in which case, Recall should be high so that there are no misses.  Often, a high F1 score strikes a balance between the two.  Accuracy is a good secondary evaluation, but of less importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect and Transform\n",
    "\n",
    "Data types\n",
    "communication format: chat, email, voice\n",
    "document, file, attachment format: pdf, docx, png\n",
    "\n",
    "international, multiple languages\n",
    "encoding of plain text\n",
    "\n",
    "ETL team performs data splitting before handover.  The preferred method is Cross Validation.  However, this is often impractical during some steps because tagged training data must be manually created.  In this case, only perform Cross Validation when it is available.  \n",
    "\n",
    "SOP is to split:\n",
    "\n",
    " * 10% is used, up-front, as our sample to verify ETL (3-4 iterations)\n",
    " * 75% is used for training\n",
    "   - chat dataset\n",
    "   - email dataset\n",
    "   - voice dataset\n",
    "   - document dataset\n",
    " * 25% is kept separate as a holdout for testing\n",
    " \n",
    " \n",
    "The 75% training can be in independent datasets, based upon source.  This helps data validation be more consistent.\n",
    "\n",
    "Data validation with 10% up-front, is a very important aspect of NLP solutions because there can be such great variety within the sources being used.  During ETL verification, the raw data must be easily accessible for comparison, in case there is any doubt.  Note any peculiarities that may become sources of bias, later.\n",
    " \n",
    "Because text data usually requires a large amount of files being delivered, incrementally, it may be necessary to begin to explore and summarize, and even train models, as the data is still arriving.  This has the danger of introducing bias, but it may be necessary from as business perspective.  One approach for dealing with this is [here]( {{< ref \"/posts/blog_math-reuse_tng_sample.md\" >}}).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Process\n",
    "\n",
    "Summarizing\n",
    "\n",
    "* word occurence\n",
    "* TF-IDF\n",
    "* n-grams\n",
    "\n",
    "Perform manual review of messages to get feel for culture and style.\n",
    "\n",
    "Identify opportunities to filter-out records based tags or metadata.\n",
    "\n",
    "Pay particular focus on the metadata, reply chains, and disclaimers to identify changes. \n",
    "\n",
    "\n",
    "Comparing with text used in pre-training, methods [here]( {{< ref \"/posts/blog_page-todo.md\" >}}).  Similar comparisons:\n",
    "\n",
    "* among data sources\n",
    "* within data source, across time intervals\n",
    "\n",
    "\n",
    "pre-processing, type of feature engineering, aspects that effect the model\n",
    "\n",
    "* lower-case, stop word removal, etc\n",
    "* auto-gen: disclaimers, newsletter, spam, etc\n",
    "* selection of clean sentence\n",
    "* contacts in-scope\n",
    "* metadata\n",
    "\n",
    "Apply pre-trained models, before training on the target dataset.  Evaluation is important for current and future use.\n",
    "\n",
    "Balancing the dataset through under-sampling the negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build\n",
    "\n",
    "Regex \n",
    "\n",
    "Lexicon testing:  Apply converted lexicons and test performance on the customer data\n",
    "Apply the lexicon to the non-holdout data\n",
    "Record hits\n",
    "Compare to \"gold standard\" hits from other system (all lexicons, all participants, subset of timeframe)\n",
    "Revise lexicons \n",
    "Rerun\n",
    "Uplift - use news/spam/reply chain analysis to reduce false positive\n",
    "\n",
    "\n",
    "Models\n",
    "\n",
    "Manually creating tagged training data is the most time-intensive aspect.\n",
    "\n",
    "\n",
    "Iterations\n",
    "\n",
    "* INITIAL LOOK\n",
    "  - initial run on OOB models on partial chat and email (because didn't have all data)\n",
    "  - review and assess data contents. plan accordingly \n",
    "* CHAT TUNING\n",
    "  - begin model training in Cognition using chat training data\n",
    "  - iterate over chat training KG 2x\n",
    "* EMAIL TUNING\n",
    "  - apply models to partial email training set (1.8m emails)\n",
    "  - tune models and engine filtering based on findings (note: this process was ridiculously truncated)\n",
    "* DEMO\n",
    "  - run updated engine on chat training and email training set (1.8m emails)\n",
    "  - review ~6% of the alerts. triage. select a sub-set of best alerts to demo\n",
    "\n",
    "....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
