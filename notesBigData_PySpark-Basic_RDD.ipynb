{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to spark configuration can be found, [here](https://spark.apache.org/docs/latest/configuration.html).\n",
    "\n",
    "local[K,F] : Run Spark locally with K worker threads and F maxFailures (see spark.task.maxFailures for an explanation of this variable. Number of failures of any particular task before giving up on the job. The total number of failures spread across different tasks will not cause the job to fail.\n",
    "\n",
    "local[*] : Run Spark locally with as many worker threads as logical cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting functions\n",
    "def fCtxSummary(context):\n",
    "    print(\"SUMMARY\",\n",
    "      \"\\nSpark version: {}\".format(context.version),\n",
    "      \"\\nPython version: {}\".format(context.pythonVer),\n",
    "      \"\\nMaster url: {}\".format(context.master)\n",
    "     )\n",
    "    print(\"\\n\")\n",
    "    print(\"CONFIG\",\n",
    "      \"\\nWorker node path: {}\".format( str(context.sparkHome) ), \n",
    "      \"\\nUser name: {}\".format( str(context.sparkUser() )),\n",
    "      \"\\nApp name: {}\".format( str(context.appName )),\n",
    "      \"\\nApp id: {}\".format( context.applicationId ),\n",
    "      \"\\nDefault parallelism: {}\".format( context.defaultParallelism ),\n",
    "      \"\\nMin rdd partitions: {}\".format( context.defaultMinPartitions )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    "            .setMaster(\"local[*]\")\n",
    "            .setAppName(\"My app\")\n",
    "            .set(\"spark.executor.memory\",\"1g\"))\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY \n",
      "Spark version: 2.3.1 \n",
      "Python version: 3.6 \n",
      "Master url: local[*]\n",
      "\n",
      "\n",
      "CONFIG \n",
      "Worker node path: None \n",
      "User name: jovyan \n",
      "App name: My app \n",
      "App id: local-1541700632731 \n",
      "Default parallelism: 4 \n",
      "Min rdd partitions: 2\n"
     ]
    }
   ],
   "source": [
    "fCtxSummary(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark: /usr/local/spark\n"
     ]
    }
   ],
   "source": [
    "! whereis spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/usr/local/spark')\n",
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.2 s ± 440 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# Test\n",
    "from random import random\n",
    "def sample(p):\n",
    "    x,y = random(), random()\n",
    "    return 1 if x*x + y*y <1 else 0\n",
    "\n",
    "count = sc.parallelize(range(0,10000000)).map(sample).reduce(lambda a,b: a+b)\n",
    "sc.master, 4.0 * count / 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# Basics of Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting functions\n",
    "def fRddSummary(rdd):\n",
    "    print(\"Partitions: {}\".format( rdd.getNumPartitions() ),\n",
    "      \"\\nNumber of records (file lines): {}\".format(rdd.count())\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<br>\n",
    "\n",
    "### Getting Data into RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New RDD, programmatically\n",
    "# list of tuples\n",
    "rdd_prgm_map = sc.parallelize([('a',2),('a',3),('d',1),('b',1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sequence of values\n",
    "rdd_prgm = sc.parallelize( range(0,10) )\n",
    "rdd_prgm = sc.range(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 4 \n",
      "Number of records (file lines): 10\n"
     ]
    }
   ],
   "source": [
    "fRddSummary(rdd_prgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_prgm.sum()       # sum elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 3, 'd': 1, 'b': 1}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_prgm_map.collectAsMap()   #return as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# New RDD, typical way (combine text data across files)\n",
    "rdd_files = sc.textFile(\"file:///usr/local/spark/licenses/\")\n",
    "  #files = sc.textFile(\"file:///licenses/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 38 \n",
      "Number of records (file lines): 1131\n"
     ]
    }
   ],
   "source": [
    "fRddSummary(rdd_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# New RDD, k-v pairs of docname-text, not so common\n",
    "rdd_pairs = sc.wholeTextFiles(\"file:///usr/local/spark/licenses/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 2 \n",
      "Number of records (file lines): 38\n"
     ]
    }
   ],
   "source": [
    "fRddSummary(rdd_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file:/usr/local/spark/licenses/LICENSE-netlib.txt',\n",
       " 'file:/usr/local/spark/licenses/LICENSE-jpmml-model.txt',\n",
       " 'file:/usr/local/spark/licenses/LICENSE-scalacheck.txt',\n",
       " 'file:/usr/local/spark/licenses/LICENSE-d3.min.js.txt']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys \n",
    "names = rdd_pairs.keys()\n",
    "names.collect()[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Copyright (c) 2013 Samuel Halliday\\nCopyright (c) 1992-2011 The University of Tennessee and The Unive'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# values\n",
    "filedata = rdd_pairs.values()\n",
    "filedata.collect()[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Fundamentals of partitions\n",
    "\n",
    "[how to determine when to reshuffle](https://stackoverflow.com/questions/33505050/how-to-know-when-to-repartition-coalesce-rdd-with-unbalanced-partitions-without?noredirect=1&lq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_files.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performs full shuffle to provide evenly divided data, which is needed after filtering out large amounts\n",
    "new_rdd = rdd_files.repartition(40)    # new RDD with N partitions.  only use for increasing\n",
    "new_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[143] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd.unpersist()                   # remove RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd = rdd_files.coalesce(2)       # decrease the number of partitions in the RDD to N, avoids full reshuffle\n",
    "new_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[different storage configurations](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.StorageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_files.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[get partition sizes](https://stackoverflow.com/questions/41068112/spark-find-each-partition-size-for-rdd?noredirect=1&lq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Parition Size:  2 . Max Parition Size:  3 . Avg Parition Size:  2.5 . Total Partitions:  4\n"
     ]
    }
   ],
   "source": [
    "l = rdd.glom().map(len).collect()  # get length of each partition\n",
    "print('Min Parition Size: ',min(l),'. Max Parition Size: ', max(l),'. Avg Parition Size: ', sum(l)/len(l),'. Total Partitions: ', len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[154] at RDD at PythonRDD.scala:49\n"
     ]
    }
   ],
   "source": [
    "l = rdd.mapPartitions(lambda it: [sum(1 for _ in it)])\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to repartition by a range(index)](https://stackoverflow.com/questions/30995699/how-to-define-partitioning-of-dataframe?rq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val partitionedByRange = df.repartitionByRange(42, $\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_prgm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 10, mean: 4.5, stdev: 2.87228132327, max: 9.0, min: 0.0)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_prgm.sum()\n",
    "rdd_prgm.max()\n",
    "rdd_prgm.mean()\n",
    "rdd_prgm.stdev()\n",
    "rdd_prgm.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 3, 6, 9], [3, 3, 4])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_prgm.histogram(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Selecting data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_prgm.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 8, 7]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_prgm.take(5)\n",
    "rdd_prgm.top(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Copyright (c) 2013 Samuel Halliday'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_files.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_prgm.sample(False, 0.15, 81).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_prgm.filter(lambda x: x%2).collect()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('a', 3)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_prgm_map.filter(lambda x: 'a' in x).collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1), ('a', 3), ('d', 1)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_prgm_map.distinct().collect()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Applying Function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 2), ('a', 7)]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortBy(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2), ('b', 2)]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x): print('hi!')\n",
    "rdd.foreach(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7, 7, 'a'), ('a', 2, 2, 'a'), ('b', 2, 2, 'b')]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x+(x[1],x[0])).collect()        # apply a function to each RDD element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 'a'), (2, 'a'), (2, 'b')]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: (x[1],x[0]) ).collect()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 'a', 2, 'a', 2, 'b']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: (x[1],x[0]) ).collect()            # flatten result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\",[\"x\",\"y\",\"z\"]),(\"b\",[\"p\", \"r\"])])\n",
    "rdd4.flatMapValues(lambda x: x).collect()   # apply a flatMap function to each (key,value) without changing keys                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reshaping Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 9)]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda x,y : x+y).collect()      # sum values across keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 7, 'a', 2, 'b', 2)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(lambda k, v: k+v)                     # merge keys and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [0, 2, 4]), (1, [1, 3])]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupBy(lambda x: x%2).mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', [2]), ('a', [7, 2])]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fold](https://stackoverflow.com/questions/29150202/pyspark-fold-method-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(5))\n",
    "rdd.fold(0, lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 9)]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd.foldByKey(0, lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 0), ('1', 1), ('2', 2), ('3', 3), ('4', 4)]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(5))\n",
    "rdd.keyBy(lambda x: str(x)).collect()                # create tuples of RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(5))\n",
    "seqOp = (lambda x,y: (x[0]+y,x[1]+1))\n",
    "combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "rdd.aggregate((0,0),seqOp,combOp)    # Aggregate RDD elements of each partition and then the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Example: Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1131"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_files.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "flattend = files.filter(lambda line: len(line)>0 ) \\\n",
    "                .flatMap(lambda line: re.split('\\W+', line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Copyright', 'c', '2013', 'Samuel', 'Halliday', 'Copyright']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattend.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kvpairs = flattend.filter(lambda word: len(word)>0) \\\n",
    "                  .map(lambda word:(word.lower(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('copyright', 1),\n",
       " ('c', 1),\n",
       " ('2013', 1),\n",
       " ('samuel', 1),\n",
       " ('halliday', 1),\n",
       " ('copyright', 1)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kvpairs.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "countsbyword = kvpairs.reduceByKey(lambda v1, v2: v1+v2)  \\\n",
    "                      .sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zstd', 2),\n",
       " ('zstandard', 1),\n",
       " ('zope', 3),\n",
       " ('zeiger', 1),\n",
       " ('your', 1),\n",
       " ('you', 1)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countsbyword.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "topwords = countsbyword.map(lambda wc: (wc[1],wc[0]) )  \\\n",
    "                       .sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(612, 'the'), (476, 'of'), (445, 'or'), (329, 'and'), (257, 'in')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topwords.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "topwords.saveAsTextFile(\"/usr/local/spark-2.1.0-bin-hadoop2.7/data/wordcounts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# SQLContext\n",
    "\n",
    "[ref](http://nbviewer.jupyter.org/github/tfolkman/learningwithdata/blob/master/Getting_Started_With_Spark_DataFrame.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "req = requests.get('https://data.ny.gov/resource/unag-2p27.json?$limit=50000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "content = json.loads(req.text)\n",
    "with open(\"../data/ny_salaries.json\", \"w\") as f:\n",
    "    json.dump(content, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompTot-Env.rda  ny_salaries.json  prjCCppJavaCshVb.db\n"
     ]
    }
   ],
   "source": [
    "! ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.json(\"../data/ny_salaries.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actual_salary_paid: string (nullable = true)\n",
      " |-- authority_name: string (nullable = true)\n",
      " |-- base_annualized_salary: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- exempt_indicator: string (nullable = true)\n",
      " |-- extra_pay: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- fiscal_year_end_date: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- has_employees: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- middle_initial: string (nullable = true)\n",
      " |-- other_compensation: string (nullable = true)\n",
      " |-- overtime_paid: string (nullable = true)\n",
      " |-- paid_by_another_entity: string (nullable = true)\n",
      " |-- paid_by_state_or_local_government: string (nullable = true)\n",
      " |-- pay_type: string (nullable = true)\n",
      " |-- performance_bonus: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- total_compensation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|actual_salary_paid|\n",
      "+------------------+\n",
      "|          28725.55|\n",
      "|          31973.09|\n",
      "|          68654.00|\n",
      "+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('actual_salary_paid').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+----------------------+----------+----------------+---------+----------+--------------------+----------+-------------+---------+--------------+------------------+-------------+----------------------+---------------------------------+--------+-----------------+------------------+------------------+\n",
      "|actual_salary_paid|      authority_name|base_annualized_salary|department|exempt_indicator|extra_pay|first_name|fiscal_year_end_date|     group|has_employees|last_name|middle_initial|other_compensation|overtime_paid|paid_by_another_entity|paid_by_state_or_local_government|pay_type|performance_bonus|             title|total_compensation|\n",
      "+------------------+--------------------+----------------------+----------+----------------+---------+----------+--------------------+----------+-------------+---------+--------------+------------------+-------------+----------------------+---------------------------------+--------+-----------------+------------------+------------------+\n",
      "|         118890.00|Albany Convention...|             118890.00|      null|               Y|     0.00|    Duncan| 2011-12-31T00:00:00|Managerial|         null|  Stewart|          null|              0.00|         0.00|                     N|                             null|      FT|             0.00|Executive Director|         118890.00|\n",
      "|         118890.00|Albany Convention...|             118890.00|      null|               Y|     0.00|    Duncan| 2012-12-31T00:00:00|Managerial|         null|  Stewart|          null|              0.00|         0.00|                     N|                             null|      FT|             0.00|Executive Director|         118890.00|\n",
      "|         118890.00|Albany Convention...|             118890.00|      null|               Y|     0.00|    Duncan| 2013-12-31T00:00:00|Managerial|         null|  Stewart|          null|              0.00|         0.00|                     N|                             null|      FT|             0.00|Executive Director|         118890.00|\n",
      "+------------------+--------------------+----------------------+----------+----------------+---------+----------+--------------------+----------+-------------+---------+--------------+------------------+-------------+----------------------+---------------------------------+--------+-----------------+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"total_compensation\"] > 100000).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "df_cast = df.withColumn(\"total_compensation_float\", df.total_compensation.astype(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total_compensation_float',\n",
       " 'total_compensation',\n",
       " 'title',\n",
       " 'performance_bonus',\n",
       " 'pay_type']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(df_cast.columns))[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "top10 = (df_cast\n",
    "         .groupBy(\"department\")\n",
    "         .mean(\"total_compensation_float\")\n",
    "         .orderBy(desc(\"AVG(total_compensation_float)\"))\n",
    "         .take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(department='8705 Bariatrics', avg(total_compensation_float)=332580.46875),\n",
       " Row(department='BARIATRICS', avg(total_compensation_float)=285044.1979166667),\n",
       " Row(department='DR. SPERRY', avg(total_compensation_float)=279918.5),\n",
       " Row(department='8720 Clinical Medicine', avg(total_compensation_float)=262896.0859375),\n",
       " Row(department='8715 Oral Oncology Maxolfacial', avg(total_compensation_float)=228185.66666666666),\n",
       " Row(department='ORAL ONCOLOGY & MAXOFACIAL', avg(total_compensation_float)=216544.56640625),\n",
       " Row(department='PHYSICIANS - CARDIOL', avg(total_compensation_float)=214361.0),\n",
       " Row(department='OFFICE OF THE SR. VI', avg(total_compensation_float)=214259.40625),\n",
       " Row(department='Office of the Chief Executive', avg(total_compensation_float)=212708.86979166666),\n",
       " Row(department='CL-CONNECTIVE TIS.', avg(total_compensation_float)=210000.0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunning Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references:\n",
    "* [one](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/)\n",
    "* [two](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/)\n",
    "* [shuffle](http://blog.cloudera.com/blog/2015/05/working-with-apache-spark-or-how-i-learned-to-stop-worrying-and-love-the-shuffle/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Fundamental Exercises\n",
    "\n",
    "[ref](https://github.com/jadianes/spark-py-notebooks)\n",
    "[ref](https://github.com/MingChen0919/learning-apache-spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
