{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.__version__, pd.__version__\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#pd.set_option('display.height', 500)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "\n",
    "file1 = pd.read_csv(\"Data/survey/File1.csv\", header=\"infer\", index_col=False)\n",
    "file2 = pd.read_csv(\"Data/survey/File2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode response\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "file1['resp'] = enc.fit_transform(file1.SPENDINGRESPONSE)\n",
    "resp = file1[['ID','resp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.count_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 149)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#correct feature datatypes\n",
    "\n",
    "feat = file2\n",
    "#assume no problem with type conversion, with more time I would be more careful\n",
    "cont_var = ['f2']\n",
    "feat[cont_var] = feat[cont_var].astype(\"float64\")\n",
    "categ_var = ['f93', 'f94']\n",
    "feat[categ_var] = feat[categ_var].astype(\"category\")\n",
    "\n",
    "#assume no problem with type conversion, with more time I would be more careful\n",
    "categ = feat.select_dtypes(['object']).apply(pd.Series.astype, dtype='category')\n",
    "feat2 = feat.loc[:, feat.dtypes != np.object]\n",
    "feat3 = feat2.join(categ)\n",
    "feat3.shape    #chk: 24 + 126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge, under-sample, train-test split\n",
    "\n",
    "Survey = resp.merge(feat3, left_on=\"ID\", right_on=\"ID\", how=\"left\")\n",
    "Survey.drop(columns=['ID'])\n",
    "\n",
    "LessGov = Survey.loc[Survey.resp==1,].sample(n=6169, replace=True, random_state=1)\n",
    "MoreGov = Survey.loc[Survey.resp==0,]\n",
    "Survey = pd.concat([LessGov, MoreGov]).reset_index(drop=True)\n",
    "\n",
    "y_set = Survey[['resp']] \n",
    "X_set = Survey.drop(columns=['ID','resp'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_set, y_set, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "#separate encode numeric, categoric\n",
    "#perform pca\n",
    "#run randomforest() in cv\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "colname_num = (X_train.loc[:, X_train.dtypes == np.float64]).columns\n",
    "colname_cat = (X_train.loc[:, X_train.dtypes != np.float64]).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, colname_num),\n",
    "        ('cat', categorical_transformer, colname_cat)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_gs.get_params().keys()\n",
    "#import sklearn\n",
    "#sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num', Pipeline(memory=None,\n",
       "     steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "       strategy='median', verbo...owski',\n",
       "           metric_params=None, n_jobs=5, n_neighbors=5, p=2,\n",
       "           weights='uniform'))]),\n",
       "          fit_params=None, iid=True, n_iter=20, n_jobs=None,\n",
       "          param_distributions={'knn__n_neighbors': [3, 5, 7]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs=5)\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('knn', knn)])\n",
    "param_grid = {\"knn__n_neighbors\": [3, 5, 7],\n",
    "              'preprocessor__num__pca__n_components': [10, 20, 30]}\n",
    "knn_gs = RandomizedSearchCV(pipe, param_grid, iid=True, cv=5, scoring=\"f1\", n_iter=20)\n",
    "knn_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__n_neighbors': 3}\n",
      "Best parameter (CV score=0.430):\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "\n",
    "knn_best = knn_gs.best_estimator_\n",
    "print(knn_gs.best_params_)\n",
    "#GridSearchCV: Mean cross-validated score of the best_estimator\n",
    "#SGDClassifier: Returns the mean accuracy on the given test data and labels\n",
    "print(\"Best parameter (CV score=%0.3f):\" % knn_gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9763, 1290],\n",
       "       [1259, 3688]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_pred = knn_gs.predict(X_train)\n",
    "print( \"Confusion matrix: Training\")\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_gs.predict(X_test)\n",
    "print( \"Confusion matrix: Testing\")\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.678\n",
      "Precision score: 0.648\n",
      "Recall score: 0.774\n",
      "F1 score: 0.705\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# fraction of correctly classified samples\n",
    "print( \"Accuracy score: %0.3f\" % accuracy_score(y_test, y_pred) )\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "print( \"Precision score: %0.3f\" % precision_score(y_test, y_pred) )             # TP / (TP+FP)\n",
    "print( \"Recall score: %0.3f\" % recall_score(y_test, y_pred)   )                 # TP / (TP+FN)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print( \"F1 score: %0.3f\" % f1_score(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training probability: 0.568\n",
      "Area Under ROC Curve: 0.737\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "#predictions for outcome labels\n",
    "#Predict class probabilities for X.  The predicted class probabilities of an input sample are computed as the mean predicted\n",
    "#class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same\n",
    "#class in a leaf.\n",
    "y_class_prob = knn_gs.predict_proba(X_test)                 # called   predict_proba(), for some classifiers\n",
    "y_prob = np.asarray( [x[1] for x in y_class_prob], dtype=np.float32)\n",
    "threshold = 0                                               # set threshold\n",
    "y_some_digit_pred = (y_prob > threshold)\n",
    "print( \"Average training probability: %0.3f\" % np.mean(y_prob) )\n",
    "\n",
    "#roc auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print( \"Area Under ROC Curve: %0.3f\" % roc_auc_score(y_test, y_prob) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num', Pipeline(memory=None,\n",
       "     steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "       strategy='median', verbo...dom_state=0, shuffle=True, tol=1e-05,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=None,\n",
       "       param_grid={'preprocessor__num__pca__n_components': [10, 20, 30], 'logistic__alpha': array([1.e-04, 1.e-02, 1.e+00, 1.e+02, 1.e+04])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ref: SGD classifier will have the same loss function as the Logistic Regression but a different solver\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "logistic = SGDClassifier(loss='log', penalty='l1', early_stopping=True,\n",
    "                         max_iter=10000, tol=1e-5, random_state=0)\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('logistic', logistic)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__pca__n_components': [10, 20, 30],\n",
    "    'logistic__alpha': np.logspace(-4, 4, 5),\n",
    "}\n",
    "log_gs = GridSearchCV(pipe, param_grid, iid=True, cv=10, \n",
    "                      return_train_score=False,\n",
    "                     scoring='f1')\n",
    "log_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logistic__alpha': 0.0001, 'preprocessor__num__pca__n_components': 10}\n",
      "Best parameter (CV score=0.346):\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "\n",
    "log_best = log_gs.best_estimator_\n",
    "print(log_gs.best_params_)\n",
    "#GridSearchCV: Mean cross-validated score of the best_estimator\n",
    "#SGDClassifier: Returns the mean accuracy on the given test data and labels\n",
    "print(\"Best parameter (CV score=%0.3f):\" % log_gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7113, 3940],\n",
       "       [2576, 2371]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_train_pred = log_gs.predict(X_train)\n",
    "print( \"Confusion matrix: Training\")\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: Testing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1810,  968],\n",
       "       [ 656,  566]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = log_gs.predict(X_test)\n",
    "print( \"Confusion matrix: Testing\")\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.594\n",
      "Precision score: 0.369\n",
      "Recall score: 0.463\n",
      "F1 score: 0.411\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = log_gs.predict(X_test)\n",
    "# fraction of correctly classified samples\n",
    "print( \"Accuracy score: %0.3f\" % accuracy_score(y_test, y_pred) )\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "print( \"Precision score: %0.3f\" % precision_score(y_test, y_pred) )             # TP / (TP+FP)\n",
    "print( \"Recall score: %0.3f\" % recall_score(y_test, y_pred)   )                 # TP / (TP+FN)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print( \"F1 score: %0.3f\" % f1_score(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training probability: -0.769\n",
      "Area Under ROC Curve: 0.582\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "#predictions for outcome labels\n",
    "#decision_function: Predict confidence scores for samples.  The confidence score for a sample is the signed distance of that\n",
    "#sample to the hyperplane.\n",
    "y_prob = log_gs.decision_function(X_test)\t\t        # called   predict_proba(), for some classifiers\n",
    "threshold = 0\t\t\t\t\t\t\t\t\t\t\t# set threshold\n",
    "y_some_digit_pred = (y_prob > threshold)\n",
    "print( \"Average training probability: %0.3f\" % np.mean(y_prob) )\n",
    "\n",
    "#roc auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print( \"Area Under ROC Curve: %0.3f\" % roc_auc_score(y_test, y_prob) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num', Pipeline(memory=None,\n",
       "     steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "       strategy='median', verbo...ors='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=None,\n",
       "       param_grid={'preprocessor__num__pca__n_components': [10, 20, 30], 'rf__n_estimators': [10, 25, 50], 'rf__max_depth': array([4, 5, 6, 7, 8])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('rf', rf)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__pca__n_components': [10, 20, 30],\n",
    "    'rf__n_estimators': [10, 25, 50], \n",
    "    'rf__max_depth':np.arange(4, 9)}\n",
    "\n",
    "rf_gs = GridSearchCV(pipe, param_grid, iid=True, cv=5, return_train_score=False,\n",
    "                    scoring='f1')\n",
    "rf_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'preprocessor__num__pca__n_components': 30, 'rf__max_depth': 8, 'rf__n_estimators': 10}\n",
      "Best parameter (CV score=0.027):\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "\n",
    "rf_best = rf_gs.best_estimator_\n",
    "print(rf_gs.best_params_)\n",
    "#GridSearchCV: Mean cross-validated score of the best_estimator\n",
    "#SGDClassifier: Returns the mean accuracy on the given test data and labels\n",
    "print(\"Best parameter (CV score=%0.3f):\" % rf_gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[11047,     6],\n",
       "       [ 4820,   127]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_train_pred = rf_gs.predict(X_train)\n",
    "print( \"Confusion matrix: Training\")\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: Testing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2770,    8],\n",
       "       [1206,   16]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf_gs.predict(X_test)\n",
    "print( \"Confusion matrix: Testing\")\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.697\n",
      "Precision score: 0.667\n",
      "Recall score: 0.013\n",
      "F1 score: 0.026\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = rf_gs.predict(X_test)\n",
    "# fraction of correctly classified samples\n",
    "print( \"Accuracy score: %0.3f\" % accuracy_score(y_test, y_pred) )\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "print( \"Precision score: %0.3f\" % precision_score(y_test, y_pred) )             # TP / (TP+FP)\n",
    "print( \"Recall score: %0.3f\" % recall_score(y_test, y_pred)   )                 # TP / (TP+FN)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print( \"F1 score: %0.3f\" % f1_score(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training probability: 0.307\n",
      "Area Under ROC Curve: 0.665\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "#predictions for outcome labels\n",
    "#Predict class probabilities for X.  The predicted class probabilities of an input sample are computed as the mean predicted\n",
    "#class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same\n",
    "#class in a leaf.\n",
    "y_class_prob = rf_gs.predict_proba(X_test)                  # called   predict_proba(), for some classifiers\n",
    "y_prob = np.asarray( [x[1] for x in y_class_prob], dtype=np.float32)\n",
    "threshold = 0                                               # set threshold\n",
    "y_some_digit_pred = (y_prob > threshold)\n",
    "print( \"Average training probability: %0.3f\" % np.mean(y_prob) )\n",
    "\n",
    "#roc auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print( \"Area Under ROC Curve: %0.3f\" % roc_auc_score(y_test, y_prob) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
