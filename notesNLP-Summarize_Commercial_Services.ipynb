{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Natural Language Understanding Services and Frameworks\n",
    "\n",
    "This document compares the leading cloud service providers, and programming frameworks, for Natural Language Understanding (NLU).  A summary table of specific characteristics displays an overview of the tool differences.  Additional comparison tables are show commercial cloud service providers, in-depth.  The document concludes with code samples implementing the different tools.\n",
    "\n",
    "\n",
    "__Executive Summary__\n",
    "\n",
    "Open source programming frameworks compare favorably, and oftentimes dominate, commerical cloud providers in both features and performance.  Python's [spaCy](https://spacy.io/usage/facts-figures#section-benchmarks) appears to be optimally designed for production and offers a large number of features.  Only Stanford's CoreNLP has similar qualities for open software, and commercial services do not appear to offer as much.  Building a foundational layer with this library allows for strong, general, start to more specific solutions, later.  In particular, [R has over a hundred libraries](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html) that provide highly specialized functionality.\n",
    "\n",
    "Commercial services do allow for simplified pricing because costs are per NLU item, not processing time, which must be calculated for manual methods.  However, using a commercial service does not obviate the need for additional programming in order to customize general and specific solutions.  In which case, the commercial service becomes an extra layer of complexity between processes in a pipeline.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Open source references__\n",
    "\n",
    "* [Python: spaCy](https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/)\n",
    "* [R: TextMining(tm)](https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/)\n",
    "* [R: OpenNLP](https://rpubs.com/lmullen/nlp-chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Commercial references__\n",
    "\n",
    "* [kontikilabs: very thorough with accompanying code](https://medium.com/kontikilabs/comparing-machine-learning-ml-services-from-various-cloud-ml-service-providers-63c8a2626cb6)\n",
    "* [Google vs Watson](http://fredrikstenbeck.com/google-natural-language-vs-watson-natural-language-understanding/)\n",
    "* [Watson internals](https://www.quora.com/What-do-AI-ML-and-NLP-researchers-think-of-IBM%E2%80%99s-Watson-Does-it-have-the-potential-to-make-a-huge-impact)\n",
    "* [Google: categories](https://cloud.google.com/natural-language/docs/categories)\n",
    "* [Watson: categories](https://console.bluemix.net/docs/services/natural-language-understanding/categories.html#categories-hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of all open frameworks and commercial services\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./images/Cloud_and_Open.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./images/Cloud_and_Open.png\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of open frameworks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./images/Open.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./images/Open.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of commercial cloud services\n",
    "\n",
    "Commercial features are fairly consistent across services, except for Syntax and Part-Of-Speech, which IMHO is a must have.  Google has better Syntax, POS.  Watson has nice hierarchical categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./images/CloudML_Features.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./images/CloudML_Features.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance may not be as important because we using the service as batch daily, and we are not paying for processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./images/CloudML_Performance.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./images/CloudML_Performance.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costs are also consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./images/CloudML_Cost.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./images/CloudML_Cost.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed explanation of differences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [R](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html)\n",
    "\n",
    "This example code is taken from the [blog](https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/), with code using additional libraries, from [here](https://rpubs.com/lmullen/nlp-chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Python: spaCy](https://spacy.io/usage/spacy-101)\n",
    "\n",
    "[SpaCy](https://spacy.io/usage/processing-pipelines) is clear in its documentation that it is built for general and customized pipelines.  This example code is taken from the [blog](https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/).\n",
    "\n",
    "`$ conda install spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/04/04080929/Tripadvisor_hotelreviews_Shivambansal.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice place Better than some reviews give it credit for. Overall, the rooms were a bit small but nice'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare space\n",
    "import spacy \n",
    "nlp = spacy.load('en')\n",
    "\n",
    "document = r.text\n",
    "document = nlp(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifiers in module\n",
    "dir(document)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Nice place Better than some reviews give it credit for.,\n",
       " Overall, the rooms were a bit small but nice.,\n",
       " Everything was clean, the view was wonderful and it is very well located (the Prudential Center makes shopping and eating easy and the T is nearby for jaunts out and about the city).,\n",
       " Overall, it was a good experience and the staff was quite friendly. ,\n",
       " what a surprise What a surprise the Sheraton was after reading some of the reviews.]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "document[0]\n",
    "document[len(document)-5]\n",
    "list(document.sents)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech\n",
    "all_tags = {w.pos: w.pos_ for w in document}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{82: 'ADJ',\n",
       " 83: 'ADP',\n",
       " 84: 'ADV',\n",
       " 87: 'CCONJ',\n",
       " 88: 'DET',\n",
       " 89: 'INTJ',\n",
       " 90: 'NOUN',\n",
       " 91: 'NUM',\n",
       " 92: 'PART',\n",
       " 93: 'PRON',\n",
       " 94: 'PROPN',\n",
       " 95: 'PUNCT',\n",
       " 97: 'SYM',\n",
       " 98: 'VERB',\n",
       " 99: 'X',\n",
       " 101: 'SPACE'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice JJ\n",
      "place NN\n",
      "Better NNP\n",
      "than IN\n",
      "some DT\n",
      "reviews NNS\n",
      "give VBP\n",
      "it PRP\n",
      "credit NN\n",
      "for IN\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "# all tags of first sentence of our document \n",
    "for word in list(document.sents)[0]:  \n",
    "    print( word, word.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hotel', 685),\n",
       " ('room', 653),\n",
       " ('great', 300),\n",
       " ('sheraton', 286),\n",
       " ('location', 272)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define some parameters  \n",
    "noisy_pos_tags = ['PROP']\n",
    "min_token_length = 2\n",
    "\n",
    "#Function to check if the token is a noise or not  \n",
    "def isNoise(token):     \n",
    "    is_noise = False\n",
    "    if token.pos_ in noisy_pos_tags:\n",
    "        is_noise = True \n",
    "    elif token.is_stop == True:\n",
    "        is_noise = True\n",
    "    elif len(token.string) <= min_token_length:\n",
    "        is_noise = True\n",
    "    return is_noise \n",
    "\n",
    "def cleanup(token, lower = True):\n",
    "    if lower:\n",
    "       token = token.lower()\n",
    "    return token.strip()\n",
    "\n",
    "\n",
    "# top unigrams used in the reviews \n",
    "from collections import Counter\n",
    "cleaned_list = [cleanup(word.string) for word in document if not isNoise(word)]\n",
    "Counter(cleaned_list) .most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT ['the Hynes Convention centre', 'DIRTY Room / RUDE Staff My', 'the Body Shopy', 'New Year', 'the Olympic Trials']\n",
      "LAW ['#1', 'Room 2916', 'the Duck Tour - it', 'the USS Constitution', 'the Sheraton Boston']\n",
      "ORG ['', 'SHERATON', 'the Wrentham', 'Good Hotel', 'Whats Good']\n",
      "GPE ['the United States', 'Pizza', 'Starbucks', 'Wrentham Village -', 'Hotel']\n",
      "PRODU ['3.30pm', 'Radisson', 'Centre', '225.00', 'Suite']\n",
      "CARDI ['', '10,000', 'about 1000', '170', '9AM']\n",
      "LOC ['Fenway Park', 'the Back Bay', '', 'Charles River', 'the South End']\n",
      "MONEY ['about $40', '$109', '10 dollars', '99', '20/hr).I']\n",
      "QUANT ['10 feet', 'a ton', '27 inch', 'the airline miles', 'two feet']\n",
      "WORK_ ['The Room', 'the Back Bay', 'Wonderful Location The', 'Beautiful and the', 'a Charles River']\n",
      "TIME ['about 5 nights', 'the night', 'Later in the afternoon', 'early evening', '45 seconds']\n",
      "NORP ['American', 'Americans', 'stayThese', 'Brit', 'Priceline']\n",
      "PERCE ['20% tip', '100%', '9pm)', '50% off', 'about 20mins,']\n",
      "DATE ['the weekend', 'a couple of bucks', '15 year old', 'the end of July 2006', 'the next day']\n",
      "FAC ['Fenway Park', '', 'the North Tower', 'the Prudential Building', 'a South Tower']\n",
      "ORDIN ['Secondly', '10th', '8th', '50th', '15-minute']\n",
      "LANGU ['English']\n",
      "PERSO ['Yahoo', 'Starbucks', 'Comfortable Beds', 'Ié\\x88¥æª\\x9de', 'Staff']\n"
     ]
    }
   ],
   "source": [
    "# entities\n",
    "labels = set([w.label_ for w in document.ents]) \n",
    "for label in labels: \n",
    "    entities = [cleanup(e.string, lower=False) for e in document.ents if label==e.label_] \n",
    "    entities = list(set(entities)) \n",
    "    print( label[:5],entities[:5] )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A :  []\n",
      "cab :  [A, from]\n",
      "from :  [airport, to]\n",
      "the :  []\n",
      "airport :  [the]\n",
      "to :  [hotel]\n",
      "the :  []\n",
      "hotel :  [the]\n",
      "can :  []\n",
      "be :  [cab, can, cheaper, .]\n",
      "cheaper :  [than]\n",
      "than :  [shuttles]\n",
      "the :  []\n",
      "shuttles :  [the, depending]\n",
      "depending :  [time]\n",
      "what :  []\n",
      "time :  [what, of]\n",
      "of :  [day]\n",
      "the :  []\n",
      "day :  [the, go]\n",
      "you :  []\n",
      "go :  [you]\n",
      ". :  []\n"
     ]
    }
   ],
   "source": [
    "# dependency parsing\n",
    "# extract all review sentences that contains the term - hotel\n",
    "hotel = [sent for sent in document.sents if 'hotel' in sent.string.lower()]\n",
    "\n",
    "# create dependency tree\n",
    "sentence = hotel[2] \n",
    "for word in sentence:\n",
    "    print( word, ': ', str(list(word.children)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 368),\n",
       " ('other', 266),\n",
       " ('my', 247),\n",
       " ('our', 243),\n",
       " ('nice', 228),\n",
       " ('good', 223),\n",
       " ('that', 181),\n",
       " ('many', 155),\n",
       " ('its', 145),\n",
       " ('which', 142)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all adjectives used with a word \n",
    "def pos_words (sentence, token, ptag):\n",
    "    sentences = [sent for sent in sentence.sents if token in sent.string]     \n",
    "    pwrds = []\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            for character in word.string: \n",
    "                   pwrds.extend([child.string.strip() for child in word.children if child.pos_ == ptag] )\n",
    "    return Counter(pwrds).most_common(10)\n",
    "\n",
    "pos_words(document, 'hotel', 'ADJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Scala: Epic](http://www.scalanlp.org/documentation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [IBM Watson](https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 \\\n",
    "  import Features, EntitiesOptions, KeywordsOptions\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "#We need to get our API credentials in the code for authentication that we have stored as Environment Variables locally\n",
    "NLP_USER_WATSON = os.environ.get(\"NLP_USER_WATSON\")\n",
    "NLP_PASS_WATSON = os.environ.get(\"NLP_PASS_WATSON\")\n",
    "NLP_VER_WATSON = os.environ.get(\"NLP_VER_WATSON\")\n",
    "\n",
    "\n",
    "#Following line is used to save all the console output into a text file\n",
    "sys.stdout = open('nlp_api_output.txt', 'a')\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "\n",
    "def input_file(text_file_path):\n",
    "    global text\n",
    "    if os.path.isfile(text_file_path):\n",
    "        with open(text_file_path, 'r') as text_file:\n",
    "            text = text_file.read()\n",
    "    else:\n",
    "        print(\"File doesn't exist in the directory!\")\n",
    "\n",
    "\n",
    "def analyze_text():\n",
    "  #Initialize NaturalLanguageUnderstanding function using the API credentials\n",
    "  natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    username = NLP_USER_WATSON,\n",
    "    password = NLP_PASS_WATSON,\n",
    "    version = NLP_VER_WATSON)\n",
    "\n",
    "  response = natural_language_understanding.analyze(\n",
    "    text = text,\n",
    "    features = Features(\n",
    "      entities = EntitiesOptions(\n",
    "        emotion = True,\n",
    "        sentiment = True),\n",
    "      keywords = KeywordsOptions(\n",
    "        emotion = True,\n",
    "        sentiment = True)))\n",
    "\n",
    "  print(json.dumps(response, indent = 2)) #json output after textual analysis\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description = __doc__,\n",
    "        formatter_class = argparse.RawDescriptionHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        'text_file_path',\n",
    "        help = 'The complete file path of the text file you want to analyze.') \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    input_file(args.text_file_path)\n",
    "    analyze_text()\n",
    "\n",
    "\n",
    "end_time = time.monotonic()\n",
    "print(\"Execution_Time:\", timedelta(seconds = end_time - start_time))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Google Cloud Natural Language](https://cloud.google.com/natural-language/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "\n",
    "#We need to get our API credentials in the code for authentication that we have stored as Environment Variables locally.\n",
    "os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "#Following line is used to save all the console outputs in a text file.\n",
    "sys.stdout = open('nlp_api_content_output.txt', 'w')\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "\n",
    "def input_file(text_file_path):\n",
    "    global text\n",
    "    if os.path.isfile(text_file_path):\n",
    "        with open(text_file_path, 'r') as text_file:\n",
    "            text = text_file.read()\n",
    "    else:\n",
    "        print(\"File doesn't exist in the directory!\")\n",
    "\n",
    "\n",
    "def sentiment_text():\n",
    "    \"\"\"Detects sentiment in the text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "    # Instantiates a plain text document.\n",
    "    document = types.Document(\n",
    "        content = text,\n",
    "        type = enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detects sentiment in the document. You can also analyze HTML with:\n",
    "    #   document.type == enums.Document.Type.HTML\n",
    "    sentiment = client.analyze_sentiment(document).document_sentiment\n",
    "\n",
    "    print('Sentiment: {}, {}'.format(sentiment.score, sentiment.magnitude))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def entities_text():\n",
    "    \"\"\"Detects entities in the text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    document = types.Document(\n",
    "        content = text,\n",
    "        type = enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detects entities in the document. You can also analyze HTML with:\n",
    "    #   document.type == enums.Document.Type.HTML\n",
    "    entities = client.analyze_entities(document).entities\n",
    "\n",
    "    # entity types from enums.Entity.Type\n",
    "    entity_type = ('UNKNOWN', 'PERSON', 'LOCATION', 'ORGANIZATION',\n",
    "                   'EVENT', 'WORK_OF_ART', 'CONSUMER_GOOD', 'OTHER')\n",
    "\n",
    "    for entity in entities:\n",
    "        print('=' * 20)\n",
    "        print(u'{:<16}: {}'.format('name', entity.name))\n",
    "        print(u'{:<16}: {}'.format('type', entity_type[entity.type]))\n",
    "        print(u'{:<16}: {}'.format('metadata', entity.metadata))\n",
    "        print(u'{:<16}: {}'.format('salience', entity.salience))\n",
    "        print(u'{:<16}: {}'.format('wikipedia_url',\n",
    "              entity.metadata.get('wikipedia_url', '-')))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def syntax_text():\n",
    "    \"\"\"Detects syntax in the text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    document = types.Document(\n",
    "        content = text,\n",
    "        type = enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detects syntax in the document. You can also analyze HTML with:\n",
    "    #   document.type == enums.Document.Type.HTML\n",
    "    tokens = client.analyze_syntax(document).tokens\n",
    "\n",
    "    # part-of-speech tags from enums.PartOfSpeech.Tag\n",
    "    pos_tag = ('UNKNOWN', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM',\n",
    "               'PRON', 'PRT', 'PUNCT', 'VERB', 'X', 'AFFIX')\n",
    "\n",
    "    for token in tokens:\n",
    "        print(u'{}: {}'.format(pos_tag[token.part_of_speech.tag],\n",
    "                               token.text.content))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def entity_sentiment_text():\n",
    "    \"\"\"Detects entity sentiment in the provided text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    document = types.Document(\n",
    "        content = text.encode('utf-8'),\n",
    "        type = enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detect and send native Python encoding to receive correct word offsets.\n",
    "    encoding = enums.EncodingType.UTF32\n",
    "    if sys.maxunicode == 65535:\n",
    "        encoding = enums.EncodingType.UTF16\n",
    "\n",
    "    result = client.analyze_entity_sentiment(document, encoding)\n",
    "\n",
    "    for entity in result.entities:\n",
    "        print('Mentions: ')\n",
    "        print(u'Name: \"{}\"'.format(entity.name))\n",
    "        for mention in entity.mentions:\n",
    "            print(u'  Begin Offset : {}'.format(mention.text.begin_offset))\n",
    "            print(u'  Content : {}'.format(mention.text.content))\n",
    "            print(u'  Magnitude : {}'.format(mention.sentiment.magnitude))\n",
    "            print(u'  Sentiment : {}'.format(mention.sentiment.score))\n",
    "            print(u'  Type : {}'.format(mention.type))\n",
    "        print(u'Salience: {}'.format(entity.salience))\n",
    "        print(u'Sentiment: {}\\n'.format(entity.sentiment))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def classify_text():\n",
    "    \"\"\"Classifies content categories of the provided text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    document = types.Document(\n",
    "        content = text.encode('utf-8'),\n",
    "        type = enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    categories = client.classify_text(document).categories\n",
    "\n",
    "    for category in categories:\n",
    "        print(u'=' * 20)\n",
    "        print(u'{:<16}: {}'.format('name', category.name))\n",
    "        print(u'{:<16}: {}'.format('confidence', category.confidence))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description = __doc__,\n",
    "        formatter_class = argparse.RawDescriptionHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        'text_file_path',\n",
    "        help = 'The complete file path of the text file you want to analyze.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    input_file(args.text_file_path)\n",
    "    sentiment_text()\n",
    "    entities_text()\n",
    "    syntax_text()\n",
    "    entity_sentiment_text()\n",
    "    classify_text()\n",
    "\n",
    "\n",
    "end_time = time.monotonic()\n",
    "print(\"Execution_Time:\", timedelta(seconds = end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Amazon Comprehend](https://aws.amazon.com/documentation/comprehend/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "#We need to get our API credentials in the code for authentication that we have stored as Environment Variables locally.\n",
    "os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "os.environ.get(\"AWS_REGION\")\n",
    "    \n",
    "\n",
    "#Following line is used to save all the console outputs in a text file.\n",
    "sys.stdout = open('output.txt','a')\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "\n",
    "def input_file(text_file_path):\n",
    "    global text\n",
    "    if os.path.isfile(text_file_path):\n",
    "        with open(text_file_path, 'r') as text_file:\n",
    "            text = text_file.read()\n",
    "    else:\n",
    "        print(\"File doesn't exist in the directory!\")\n",
    "\n",
    "\n",
    "def dominant_language_text():\n",
    "    #Initialize amazon_comprehend client function\n",
    "    client_comprehend = boto3.client(\n",
    "        'comprehend',\n",
    "    )\n",
    "    dominant_language_response = client_comprehend.detect_dominant_language(\n",
    "        Text = text\n",
    "    )\n",
    "    #Print the Dominant Language\n",
    "    print(\"Language:\", sorted(dominant_language_response['Languages'], key = lambda k: k['LanguageCode'])[0]['LanguageCode'])\n",
    "\n",
    "\n",
    "def entities_text():\n",
    "    #Initialize amazon_comprehend client function\n",
    "    client_comprehend = boto3.client(\n",
    "        'comprehend',\n",
    "    )\n",
    "    response_entities = client_comprehend.detect_entities(\n",
    "            Text = text,\n",
    "            LanguageCode = 'en'\n",
    "    )\n",
    "    entities = list(set([obj['Type'] for obj in response_entities['Entities']]))\n",
    "    #Print the Entities\n",
    "    print(\"Entities:\",entities)\n",
    "\n",
    "\n",
    "def key_phrases_text():\n",
    "    #Initialize amazon_comprehend client function\n",
    "    client_comprehend = boto3.client(\n",
    "        'comprehend',\n",
    "    )\n",
    "    response_key_phrases = client_comprehend.detect_key_phrases(\n",
    "        Text = text,\n",
    "        LanguageCode = 'en'\n",
    "    )\n",
    "    key_phrases = list(set([obj['Text'] for obj in response_key_phrases['KeyPhrases']]))\n",
    "    #Print the Key Phrases\n",
    "    print(\"Key Phrases:\", key_phrases)\n",
    "\n",
    "\n",
    "def sentiment_text():\n",
    "    #Initialize amazon_comprehend client function\n",
    "    client_comprehend = boto3.client(\n",
    "        'comprehend',\n",
    "    )\n",
    "    response_sentiment = client_comprehend.detect_sentiment(\n",
    "        Text = text,\n",
    "        LanguageCode = 'en'\n",
    "    )\n",
    "    sentiment = response_sentiment['Sentiment']\n",
    "    #Print the Sentiment\n",
    "    print(\"Sentiment Analysis:\" , sentiment)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description = __doc__,\n",
    "        formatter_class = argparse.RawDescriptionHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        'text_file_path',\n",
    "        help = 'The complete file path of the text file you want to analyze.')\n",
    "    args = parser.parse_args()\n",
    "    input_file(args.text_file_path)\n",
    "    dominant_language_text()\n",
    "    entities_text()\n",
    "    key_phrases_text()\n",
    "    sentiment_text()\n",
    "\n",
    "\n",
    "end_time = time.monotonic()\n",
    "print(\"Execution_Time:\", timedelta(seconds = end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Microsoft Azure Text Analytics](https://azure.microsoft.com/en-us/resources/videos/learn-how-to-create-text-analytics-solutions-with-azure-machine-learning-templates/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport requests\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import argparse\n",
    "\n",
    "\n",
    "#We need to get our API credentials in the code for authentication that we have stored as Environment Variables locally\n",
    "Ocp_Apim_Subscription_Key = os.environ.get(\"KEY_NLP\")\n",
    "\n",
    "\n",
    "#Following line is used to save all the console output into a text file\n",
    "sys.stdout = open('nlp_api_output.txt', 'a')\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "\n",
    "def input_file(text_file_path):\n",
    "    global text\n",
    "    if os.path.isfile(text_file_path):\n",
    "        with open(text_file_path, 'r') as text_file:\n",
    "            text = text_file.read()\n",
    "    else:\n",
    "        print(\"File doesn't exist in the directory!\")\n",
    "\n",
    "\n",
    "def analyze_text():\n",
    "    headers = {\n",
    "        # NOTE: Replace the \"Ocp-Apim-Subscription-Key\" value with a valid subscription key.\n",
    "        'Ocp-Apim-Subscription-Key': Ocp_Apim_Subscription_Key,\n",
    "    }\n",
    "\n",
    "    urls = ['https://eastus2.api.cognitive.microsoft.com/text/analytics/v2.0/languages', 'https://eastus2.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment', 'https://eastus2.api.cognitive.microsoft.com/text/analytics/v2.0/keyPhrases']\n",
    "\n",
    "    documents = { 'documents': [\n",
    "        { 'id': '1', 'language': 'en', 'text': text }]}\n",
    "\n",
    "    try:\n",
    "        # NOTE: You must use the same location in your REST call as you used to obtain your subscription keys.\n",
    "        #   For example, if you obtained your subscription keys from westus, replace \"eastus2\" in the\n",
    "        #   URLs above with \"westus\".\n",
    "        for url in urls:\n",
    "            response = requests.post(url = url,\n",
    "                                 headers = headers,\n",
    "                                 data = (json.dumps(documents)).encode('utf-8'))\n",
    "            data = response.json()\n",
    "            print(data)\n",
    "        print('\\n')\n",
    "    except Exception as e:\n",
    "        print('Error: ', e)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description = __doc__,\n",
    "        formatter_class = argparse.RawDescriptionHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        'text_file_path',\n",
    "        help = 'The complete file path of the text file you want to analyze.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    input_file(args.text_file_path)\n",
    "    analyze_text()\n",
    "\n",
    "\n",
    "end_time = time.monotonic()\n",
    "print(\"Execution_Time:\", timedelta(seconds = end_time - start_time))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF DOCUMENT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
