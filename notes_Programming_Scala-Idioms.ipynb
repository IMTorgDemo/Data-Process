{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [opinionated scala](https://github.com/ghik/opinionated-scala)\n",
    "* [small bites](http://matt.might.net/articles/learning-scala-in-small-bites/)\n",
    "* [twitter scala school](https://twitter.github.io/scala_school/)     \n",
    "* [twitter effective scala](http://twitter.github.io/effectivescala/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Symbols__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aSymbol: Symbol = 'foo\n",
       "aSecSymbol: Symbol = 'bar\n",
       "res2: Boolean = false\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "Symbols are used where you have a closed set of identifiers that you want to be able to compare quickly (eg. eq)\n",
    "\n",
    "When you have two String instances they are not guaranteed to be interned[1], so to compare them you must often \n",
    "check their contents by comparing lengths and even checking character-by-character whether they are the same.  [1] Interning\n",
    "is a process whereby when you create an object, you check whether an equal one already exists, and use that one if it does.\n",
    "\n",
    "With Symbol instances, comparisons are a simple eq check (i.e. == in Java), so they are constant time (i.e. O(1)) to look up.\n",
    "*/\n",
    "val aSymbol = 'foo\n",
    "val aSecSymbol = 'bar\n",
    "aSymbol == aSecSymbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aString: String = foo\n",
       "aSecString: String = bar\n",
       "res4: Boolean = false\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aString = \"foo\"\n",
    "val aSecString = \"bar\"\n",
    "aSymbol == aSecSymbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aChar: Char = f\n",
       "aString: String = f\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aChar = 'f'\n",
    "val aString = \"f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: String = a|b|c\n",
       "res5: Array[String] = Array(a, b, c)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = \"a|b|c\"\n",
    "a.split(raw\"\\|\")\n",
    "a.split(\"\\\\|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ranges__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aRange1: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5)\n",
       "aRange2: scala.collection.immutable.Range = Range(1, 2, 3, 4)\n",
       "aRnage3: scala.collection.immutable.Range = Range(1, 3, 5)\n",
       "aRange4: scala.collection.immutable.NumericRange.Inclusive[Char] = NumericRange(a, b, c)\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aRange1 = 1 to 5\n",
    "val aRange2 = 1 until 5\n",
    "val aRnage3 = 1 to 5 by 2\n",
    "val aRange4 = 'a' to 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
      "List(1, 2, 3, 4, 5, 6, 7, 8, 9)\n"
     ]
    }
   ],
   "source": [
    "println(  (1 to 10).toList  )\n",
    "println(  List.range(1,10)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "incImplicit: (x: Int)Int\n",
       "incAnonymous: Int => Int = <function1>\n",
       "defined class Identity\n",
       "myId: Identity = Identity@440cdc92\n",
       "res29: Int = 124\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// implicit\n",
    "def incImplicit(x : Int ) = x + 1\n",
    "// anonymous\n",
    "val incAnonymous = (x : Int) => x + 1\n",
    "// class with apply method:\n",
    "class Identity {\n",
    "  def apply(x : Int) = x + 1\n",
    "}\n",
    "val myId = new Identity\n",
    "myId(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "h: (x: Int, y: Int)Int\n",
       "hC: (x: Int)(y: Int)Int\n",
       "plus3: Int => Int = <function1>\n",
       "plus_3: Int => Int = <function1>\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Multi-argument functions:\n",
    "def h(x : Int, y : Int) : Int = x + y\n",
    "// A Curried multi-argument function:\n",
    "def hC (x : Int) (y : Int) : Int = x + y\n",
    "\n",
    "// Wrong: hC 3 4\n",
    "hC (3) (4)\n",
    "// Wrong: hC (3)\n",
    "hC (3) _\n",
    "// Wrong: hC _ (4)\n",
    "hC (_:Int) (4)\n",
    "\n",
    "val plus3 = hC (_:Int) (3) \n",
    "val plus_3 = hC (3) _\n",
    "println(plus3(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "h_curry: Int => (Int => Int) = <function1>\n",
       "h_addTwo: Int => Int = <function1>\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Take any function of multiple arguments and curry it\n",
    "val h_curry = (h _).curried\n",
    "val h_addTwo = h_curry(2)\n",
    "println(h_addTwo(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a procedure.\n",
      "argless got called!\n",
      "Evaluating x\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "proc: (a: Int)Unit\n",
       "argless: Unit\n",
       "defined class LazyClass\n",
       "lc: LazyClass = LazyClass@11878370\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// A procedure:\n",
    "//Procedure syntax compiles to a method that returns unit\n",
    "def proc(a : Int) { // Implicitly : Unit\n",
    "  println(\"I'm a procedure.\")\n",
    "}\n",
    "proc(10)\n",
    "\n",
    "// An argument-less function:\n",
    "def argless : Unit = println(\"argless got called!\")\n",
    "argless\n",
    "\n",
    "// Lazy fields are argless functions that cache their result:\n",
    "class LazyClass {\n",
    "  lazy val x = { println(\"Evaluating x\") ; 3 }\n",
    "}\n",
    "val lc = new LazyClass\n",
    "println(lc.x)\n",
    "println(lc.x)\n",
    "println(lc.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArrayBuffer(One, Two, Three)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "capitalizeAll: (args: String*)Seq[String]\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Variable-length parameters\n",
    "def capitalizeAll(args: String*) = {\n",
    "  args.map { arg =>\n",
    "    arg.capitalize\n",
    "  }\n",
    "}\n",
    "println( capitalizeAll(\"one\",\"two\",\"three\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Objects and apply method__\n",
    "\n",
    "[ref: stackoverflow](https://stackoverflow.com/questions/9737352/what-is-the-apply-function-in-scala)\n",
    "\n",
    "Every function in Scala can be treated as an object and it works the other way too - every object can be treated as a function, provided it has the apply method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f: Int => Int = <function1>\n",
       "f2: Int => Int = <function1>\n",
       "res39: Int = 2\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// define a function in scala\n",
    "(x:Int) => x + 1\n",
    "// assign an object representing the function to a variable\n",
    "val f = (x:Int) => x + 1\n",
    "\n",
    "//Since everything is an object in Scala f can now be treated as a reference to Function1[Int,Int] object. \n",
    "//For example, we can call toString method inherited from Any, that would have been impossible for a pure \n",
    "//function, because functions don't have methods:\n",
    "f.toString\n",
    "\n",
    "//Or we could define another Function1[Int,Int] object by calling compose method on f and chaining two \n",
    "//different functions together:\n",
    "val f2 = f.compose((x:Int) => x - 1)\n",
    "\n",
    "//Now if we want to actually execute the function, or as mathematician say \"apply a function to its arguments\"\n",
    "//we would call the apply method on the Function1[Int,Int] object:\n",
    "f2.apply(2)\n",
    "\n",
    "//Scala compiler allows us to hide the apply call\n",
    "f2(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Many usage cases when we would want to treat an object as a function. The most common scenario is a factory pattern. \n",
    "List(1,2,3) // same as List.apply(1,2,3) but less clutter, functional notation\n",
    "\n",
    "//Factory pattern in OOP language\n",
    "List.instanceOf(1,2,3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Empty__\n",
    "\n",
    "[ref article](http://oldfashionedsoftware.com/2008/08/20/a-post-about-nothing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "// **** NEVER USE NULL ****\n",
    "//use this link for different approaches\n",
    "//   https://alvinalexander.com/scala/scala-null-values-option-uninitialized-variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it worked!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tryit: (x: Null)Unit\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Null, null\n",
    "//Null is a trait and null is its instance (the value of a reference that is not refering to any object)\n",
    "def tryit(x:Null):Unit = {println(\"it worked!\")}\n",
    "tryit(null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: String = List()1,2,3\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Nil\n",
    "//an object that extends List[Nothing] \n",
    "Nil.length\n",
    "Nil + \"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emptyStringList: List[String] = List()\n",
       "res22: String = List()sfd\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Nothing\n",
    "/*Nothing is another trait. It extends class Any. Any is the root type of the entire Scala type system.\n",
    "**There are no instances of Nothing, but (here’s the tricky bit) Nothing is a subtype of everything. \n",
    "**Nothing is a subtype of List, it’s a subtype of String, it’s a subtype of Int, it’s a subtype of YourOwnCustomClass.\n",
    "\n",
    "So Nothing is useful for defining base cases for collections\n",
    "*/\n",
    "val emptyStringList: List[String] = List[Nothing]()\n",
    "emptyStringList + \"sfd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "24: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:24: error: type mismatch;",
      " found   : String(\"abc\")",
      " required: Nothing",
      "       val emptyStringList: List[String] = List[Nothing](\"abc\")",
      "                                                         ^",
      ""
     ]
    }
   ],
   "source": [
    "//It fails because although Nothing is a subtype of everything, it isn’t a superclass of anything and there are no instances of Nothing\n",
    "val emptyStringList: List[String] = List[Nothing](\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not get a big number!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "getStringMaybe: (num: Int)Option[String]\n",
       "printResult: (num: Int)Unit\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// None\n",
    "/*What to do when you do not have a useful value to return?  Scala has a built-in solution to this problem. \n",
    "**If you want to return a String, for example, but you know that you may not be able to return a sensible value\n",
    "**you can return an Option[String]\n",
    "**\n",
    "**Option is an abstract class with exactly two subclasses, class Some and object None. Those are the only two \n",
    "**ways to instantiate an Option. So getAStringMaybe returns either a Some[String] or None. Some and None are \n",
    "**case classes, so you can use the handy match/case construct to handle the result. None is object that signifies\n",
    "**no result from the method.\n",
    "*/\n",
    "def getStringMaybe(num:Int):Option[String] = {\n",
    "    if(num > 100) return Some(\"got a big number\")\n",
    "    else return None\n",
    "}\n",
    "def printResult(num:Int) = {\n",
    "    getStringMaybe(num) match {\n",
    "        case Some(str) => println(str)\n",
    "        case None => println(\"did not get a big number!\")\n",
    "    }\n",
    "}\n",
    "printResult(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "printResult: (num: Int)Unit\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//If you simply returned null...\n",
    "//this code isn't any worse than the Scala Option and match approach, but you did have to read the Javadoc to know this was needed.\n",
    "def printResult(num:Int) = {\n",
    "    val str = getStringMaybe(num) \n",
    "    str match {\n",
    "        case null => println(\"did not get a big number!\")\n",
    "        case _ => println(str)\n",
    "    }\n",
    "}\n",
    "printResult(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toInt: (in: String)Option[Int]\n",
       "bag: List[String] = List(1, 2, foo, 3, bar)\n",
       "sum: Int = 6\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//cool use case\n",
    "//if you need access to the exception (to discover why failed), then use Either, Left, and Right\n",
    "def toInt(in: String): Option[Int] = {\n",
    "    try {\n",
    "        Some(Integer.parseInt(in.trim))\n",
    "    } catch {\n",
    "        case e: NumberFormatException => None\n",
    "    }\n",
    "}\n",
    "val bag = List(\"1\", \"2\", \"foo\", \"3\", \"bar\")\n",
    "val sum = bag.flatMap(toInt).sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "doThreeTimes: (fn: Int => Unit)Unit\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Unit\n",
    "//Unit is the type of a method that doesn’t return a value of any sort. Sound familiar? It’s like a void return type in Java. \n",
    "def doThreeTimes(fn:Int=>Unit) = {\n",
    "    fn(1); fn(2); fn(3);\n",
    "}\n",
    "doThreeTimes(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's an integer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y: Any = 10\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val y : Any = 10 \n",
    "y match {\n",
    "  case _ : String => println(\"It's a string.\")\n",
    "  case _ : Int => println(\"It's an integer.\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's foo.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class Pair\n",
       "p: Pair = Pair(42,foo)\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Case classes are matchable:\n",
    "case class Pair(val x : Int, val y : String) \n",
    "val p = Pair(42,\"foo\")\n",
    "p match {\n",
    "  case Pair(43,\"foo\") => println(\"Not me.\")\n",
    "  case Pair(42,s) => println(\"It's \" + s + \".\") \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "times: Int = 1\n",
       "res44: String = one\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Matching on values\n",
    "val times = 1\n",
    "times match {\n",
    "  case 1 => \"one\"\n",
    "  case 2 => \"two\"\n",
    "  case _ => \"some other number\"\n",
    "}\n",
    "//Matching with guards\n",
    "times match {\n",
    "  case i if i == 1 => \"one\"\n",
    "  case i if i == 2 => \"two\"\n",
    "  case _ => \"some other number\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//??? WTF ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class MyList\n",
       "defined class $colon$times$colon\n",
       "defined object MyNil\n",
       "l: MyList[Int] = :*:(3,:*:(4,:*:(5,MyNil)))\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Pattern-matchable lists can be created from scratch:\n",
    "abstract class MyList[+A] {\n",
    "  def :*: [B >: A] (head : B) = new `:*:`(head,this)\n",
    "}\n",
    "case class :*:[A](val head : A, val tail : MyList[A]) extends MyList[A] \n",
    "case object MyNil extends MyList[Nothing] \n",
    "val l : MyList[Int] = 3 :*: 4 :*: 5 :*: MyNil\n",
    "l match {\n",
    "  case hd :*: tl => println(hd)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SortedMap Orderings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.immutable.{SortedMap, TreeMap}\n",
       "defined class Person\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*Sorted maps require a comparison procedure.  Sorted data structures will use an 'implicit' function for converting\n",
    "**to Ordering if one is in scope.  If one is not in scope, it must be specified.\n",
    "*/\n",
    "import scala.collection.immutable.{SortedMap,TreeMap}\n",
    "case class Person(val ssn : Int, val name : String) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "27: error: No implicit Ordering defined for Person.",
     "output_type": "error",
     "traceback": [
      "<console>:27: error: No implicit Ordering defined for Person.",
      "       val db1: SortedMap[Person,Symbol] = TreeMap[Person,Symbol]()",
      "                                                                 ^",
      ""
     ]
    }
   ],
   "source": [
    "val db1: SortedMap[Person,Symbol] = TreeMap[Person,Symbol]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map(Person(1,Matt) -> 'Chicken, Person(2,Matt) -> 'Mouse)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined object OrderingBySSN\n",
       "db1: scala.collection.immutable.SortedMap[Person,Symbol] = Map()\n",
       "db2: scala.collection.immutable.SortedMap[Person,Symbol] = Map(Person(1,Matt) -> 'Chicken)\n",
       "db3: scala.collection.immutable.SortedMap[Person,Symbol] = Map(Person(1,Matt) -> 'Chicken, Person(2,Matt) -> 'Mouse)\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Implicitly converts a Person to an Ordered[Person], using SSNs to compare.\n",
    "implicit object OrderingBySSN extends Ordering[Person] {\n",
    "  def compare (p1: Person, p2: Person): Int = p1.ssn - p2.ssn\n",
    "}\n",
    "val db1: SortedMap[Person,Symbol] = TreeMap[Person,Symbol]()\n",
    "val db2 = db1 + ((Person(1,\"Matt\")) -> 'Chicken)\n",
    "val db3 = db2 + ((Person(2,\"Matt\")) -> 'Mouse)\n",
    "println(db3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map(Person(2,Matt) -> 'Mouse)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined object OrderingByName\n",
       "dbX: scala.collection.immutable.SortedMap[Person,Symbol] = Map()\n",
       "dbY: scala.collection.immutable.SortedMap[Person,Symbol] = Map(Person(1,Matt) -> 'Chicken)\n",
       "dbZ: scala.collection.immutable.SortedMap[Person,Symbol] = Map(Person(2,Matt) -> 'Mouse)\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Explicitly converts a Person to an Ordering[Person], using names to compare.\n",
    "object OrderingByName extends Ordering[Person] {\n",
    "  def compare (p1 : Person, p2 : Person) : Int = p1.name compare p2.name\n",
    "}\n",
    "val dbX: SortedMap[Person,Symbol] = TreeMap[Person,Symbol]()(OrderingByName)\n",
    "val dbY = dbX + ((Person(1,\"Matt\")) -> 'Chicken)\n",
    "val dbZ = dbY + ((Person(2,\"Matt\")) -> 'Mouse)\n",
    "println(dbZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object DemoApplication2\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* An object with a main method is a program. */\n",
    "object DemoApplication2 {\n",
    "  def main (args : Array[String]) {\n",
    "    println(\"Hello, World!\") \n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* If an object extends Application, then the body of the object is\n",
    "   effectively a script. */\n",
    "object DemoApplication extends Application {\n",
    "  /*\n",
    "   Extending Application runs the entire program in the constructor\n",
    "   for the object, which prevents the JVM from performing JIT\n",
    "   optimizations.\n",
    "\n",
    "   For large applications, use a main() method instead of extending\n",
    "   Application.\n",
    "  */\n",
    "  println(\"Hello, World!\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collections and FP Combinators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res59: Array[Int] = Array(1, 2, 3)\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array(1,2,3)  //mutalbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res60: List[Int] = List(1, 2, 3)\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//In Java terms, Scala's Seq would be Java's List, and Scala's List would be Java's LinkedList\n",
    "//List is fundamental for FP\n",
    "List(1,2,3)   //immutable\n",
    " 1 :: 2 :: 3 :: Nil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "//You usually should use Seq as input parameter for method or class, defined for sequences in general (just general, not necessarily with generic):\n",
    "//So now you can pass any sequence (like Vector or List) to mySort\n",
    "/*\n",
    "def mySort[T](seq: Seq[T]) = ...\n",
    "case class Wrapper[T](seq: Seq[T]) \n",
    "implicit class RichSeq[T](seq: Seq[T]) { def mySort = ...}\n",
    "*/\n",
    "collection.mutable.Seq(1,2,3)\n",
    "collection.immutable.Seq(1,2,3)  //default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res62: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3)\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vector(1,2,3)   //used for parallel programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res53: (Int, Int) = (1,5)\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1, 5)   //tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res54: (Int, Int) = (1,5)\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 -> 5   //tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num: scala.collection.immutable.Map[Int,Int] = Map(1 -> 5, 2 -> 6)\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val num = Map(1->5, 2->6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res57: Option[Int] = None\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.get(3)   //returns Option[T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Functions: Partial Functions and Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f: (s: String)String\n",
       "g: (s: String)String\n"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Compose methods\n",
    "def f(s:String) = \"f(\" + s + \")\"\n",
    "def g(s:String) = \"g(\" + s + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fComposeG: String => String = <function1>\n"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fComposeG = f _ compose g _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res82: String => String = <function1>\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// similar syntax\n",
    "(f _).compose(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res83: String = f(g(Yay))\n"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fComposeG(\"Yay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f: String => String = <function1>\n",
       "g: String => String = <function1>\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Compose functions... which have methods\n",
    "val f = (s:String) => \"f(\" + s + \")\"\n",
    "val g = (s:String) => \"g(\" + s + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res74: String => String = <function1>\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.andThen(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "one: PartialFunction[Int,String] = <function1>\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Case statements\n",
    "/* case statements are a subclass of function called a PartialFunction\n",
    "** collection of multiple case statements are multiple PartialFunctions composed together\n",
    "** isDefinedAt is a method on PartialFunction that can be used to determine if the PartialFunction will accept a given argument\n",
    "** Note PartialFunction is unrelated to a partially applied function\n",
    "*/\n",
    "val one: PartialFunction[Int, String] = {case 1 => \"one\"}\n",
    "println( one.isDefinedAt(1) )\n",
    "println( one.isDefinedAt(2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res78: String = one\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "two: PartialFunction[Int,String] = <function1>\n",
       "partial: PartialFunction[Int,String] = <function1>\n",
       "res79: String = two\n"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val two: PartialFunction[Int, String] = { case 2 => \"two\" }\n",
    "val partial = one orElse two\n",
    "partial(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class PhoneExt\n",
       "extensions: List[PhoneExt] = List(PhoneExt(steve,100), PhoneExt(robey,200))\n",
       "res80: List[PhoneExt] = List(PhoneExt(steve,100))\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//filter takes a function. In this case a predicate function of (PhoneExt) => Boolean.\n",
    "//A PartialFunction is a subtype of Function so filter can also take a PartialFunction!\n",
    "case class PhoneExt(name: String, ext: Int)\n",
    "val extensions = List(PhoneExt(\"steve\", 100), PhoneExt(\"robey\", 200))\n",
    "extensions.filter { case PhoneExt(name, extension) => extension < 200 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using spylon-kernel\n",
    "\n",
    "ERROR: https://github.com/jupyter-scala/jupyter-scala/issues/191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    ".appName(\"Spark example\")\n",
    ".master(\"local[*]\")\n",
    ".config(\"option\", \"some-value\")\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "24: error: not found: value SparkSession",
     "output_type": "error",
     "traceback": [
      "<console>:24: error: not found: value SparkSession",
      "       val spark = SparkSession.builder()",
      "                   ^",
      ""
     ]
    }
   ],
   "source": [
    "val spark = SparkSession.builder()\n",
    "//.config(\"spark.serializer\",classOf[KryoSerializer].getName)\n",
    "//.config(\"spark.kryo.registrator\", classOf[GeoSparkKryoRegistrator].getName)\n",
    ".master(\"local[*]\")\n",
    ".appName(\"Bitre\")\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://f652f65de160:4040\n",
       "SparkContext available as 'sc' (version = 2.3.1, master = local[*], app id = local-1535721809862)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-31 13:23:21 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@668644d\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: String = 2.3.1\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Spark\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "name: String = Spark\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val name = \"Spark\"\n",
    "println(s\"Hello, ${name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: Seq[(String, Int)] = List((a,0), (b,1), (c,2), (d,3))\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\"a\",\"b\",\"c\",\"d\").zip(0 to 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class scala.collection.immutable.$colon$colon\n",
      "class scala.Tuple2\n",
      "class java.lang.String\n"
     ]
    }
   ],
   "source": [
    "println(data.getClass); \n",
    "println(data(0).getClass);\n",
    "println( data(0)._1.getClass );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds = spark.createDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DataRow\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class DataRow(name:String, value:Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds2: org.apache.spark.sql.Dataset[DataRow] = [name: string, value: int]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds2 = ds.map{ x:(String, Int) => DataRow(x._1,x._2)}\n",
    "//val ds2 = ds.map{ case (a,b) => DataRow(a,b) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.sql.Dataset\n",
      "class org.apache.spark.sql.Dataset\n"
     ]
    }
   ],
   "source": [
    "println(ds2.getClass)\n",
    "println(ds2.select(\"name\").getClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- value: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds3: org.apache.spark.sql.DataFrame = [name: string, value: int]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2.createOrReplaceTempView(\"table\")\n",
    "val ds3 = spark.sql(\"SELECT * FROM table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "//spark.sql(\"SELECT * FROM table\").show()\n",
    "ds2.toDF.createOrReplaceTempView(\"table2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-31 13:34:30 ERROR Executor:91 - Exception in task 0.0 in stage 10.0 (TID 10)\n",
      "java.lang.ClassCastException: $line12.$read$$iw$$iw$DataRow cannot be cast to $line12.$read$$iw$$iw$DataRow\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2018-08-31 13:34:30 WARN  TaskSetManager:66 - Lost task 0.0 in stage 10.0 (TID 10, localhost, executor driver): java.lang.ClassCastException: $line12.$read$$iw$$iw$DataRow cannot be cast to $line12.$read$$iw$$iw$DataRow\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2018-08-31 13:34:30 ERROR TaskSetManager:70 - Task 0 in stage 10.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10, localhost, executor driver): java.lang.ClassCastException: DataRow cannot be cast to DataRow",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10, localhost, executor driver): java.lang.ClassCastException: DataRow cannot be cast to DataRow",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2484)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2698)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:723)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:682)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:691)",
      "  ... 36 elided",
      "Caused by: java.lang.ClassCastException: DataRow cannot be cast to DataRow",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:109)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
