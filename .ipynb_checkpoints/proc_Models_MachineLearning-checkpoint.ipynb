{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalizing the Machine Learning Process\n",
    "Date: 2019-05-09  \n",
    "Author: Jason Beach  \n",
    "Categories: process, data_science  \n",
    "Tags: machine_learning  \n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work describes a general approach to follow when performing machine learning (ML) manually, and when automating in a deployment setting.  Unlike a classical statistical analysis, standard machine learning projects typically follow a general and repeatable process.  While the practicioner should be aware of details for each of the steps and the reasons for choosing them, there is much less design-thinking and checking of assumptions that are necessary components of more mathematical modeling fields.  This makes the machine learning process amenable to deployment as a service because automating the re-training and prediction of a model with consistent data is straight-forward programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Theory\n",
    "\n",
    "Most of the design-thinking in the ML process is in choosing a variety of models for comparing performance against.  The following three characteristics succinctly describe a ML model.\n",
    "\n",
    "1. Representation: structural model characteristics\n",
    "    - name\n",
    "    - family\n",
    "    - interpretability\n",
    "    - type \n",
    "        + generative / discriminative \n",
    "        + bias / var\n",
    "        + fixed- / variable- learner\n",
    "2. Evaluation: functions applied to the structure\n",
    "    - objective\n",
    "    - cost\n",
    "    - loss\n",
    "3. Optimization: algorithms necessary to solve for parameters\n",
    "\n",
    "It is also important to understand how the chosen model effects the modeling process\n",
    "\n",
    "- assumptions inherent in representation\n",
    "- alignment of loss function with project goals\n",
    "- sources of bias / variance\n",
    "- determination of resource constraints\n",
    "- enumeration of how over-tuning can occur (regularization)\n",
    "- understanding when manual methods are ineffective 'fiddling' of model implementation parameters\n",
    "- statement of strong false assumptions can be better than weak true ones, because they need more data\n",
    "\n",
    "_Note:_ This should be considered carefully with feature engineering and feature selection to ensure the input transformations align with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Process\n",
    "\n",
    "The following are the general steps taken in the ML Process.  They are similar to many other problem-solving and design-thinking processes, but tailored to ML specifics.  \n",
    "\n",
    "There a several hard checks that should be used to ensure the practicioner is maintaining honesty.  One important check is laying-out proper evaluation methods, before implementing them.  This is similar to classical statistics in choosing an accepatable p-value before running the model.  \n",
    "\n",
    "Another check is on model resource and time requirements.  More sophisticated models need more memory to implement and take a longer time to run.  These are highly dependent on the environment they are deployed to.  These should be determined with the customer, at the beginning. \n",
    "\n",
    "_Discover_\n",
    "* determine problem and constraints\n",
    "* determine characteristics the problem / scenario dictates on the solution\n",
    "    - model family\n",
    "    - acceptable methods of dimensionality reduction and regularization\n",
    "    - deployment environment\n",
    "* decide evaluation\n",
    "    - primary / secondary evaluation score (ie. accuracy)\n",
    "    - methods of evaluation (ie. confusion matrix, roc)\n",
    "\n",
    "_Collect and Transform_\n",
    "* obtain raw data \n",
    "    - internal data warehouses\n",
    "    - external APIs and services\n",
    "* integration and cleaning\t\n",
    "* filter, aggregate, and query\n",
    "\n",
    "_Summary and Process_\n",
    "* exploration\n",
    "* preparation\n",
    "     - address balance (classification, anova, etc.)\n",
    "     - create Train, Validate, Test with split (above)\n",
    "* configure Feature Extraction with feature_union\n",
    "* configure Preprocess and choose model-families with pipeline\n",
    "\n",
    "_Build_\n",
    "* train the models\n",
    "    - apply k-folds CV and grid search with Training set\n",
    "    - perform on multiple model-families and hyper-parameters\n",
    "* evaluate models\n",
    "    - review afore-mentioned confusion matrix, scoring, classifier-threshold, and tests\n",
    "    - select the best model-family / hyper-parameters\n",
    "    - apply to Validation set or all of Training set to model-family to parameterize it and set as the final model\n",
    "* refine performance\n",
    "    - debug performance with learning curve, lift chart\n",
    "    - use Testing set to evaluate final model characteristics\n",
    "    - export to binary file\n",
    "\n",
    "_Deliver_\n",
    "* select solution\n",
    "    - design interface most appropriate for using the model\n",
    "    - automate data integration and pipelines\n",
    "    - implement model in deployable environment\n",
    "* deploy solution within system environments\n",
    "\n",
    "\n",
    "_Note:_ Train, Validate, and Test should be from different (independent) data sets, if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stakeholder Interaction and Timeline\n",
    "\n",
    "It is useful to display these in relation to interactions that must take place with stakeholders.  These may be business users who need a problem solved, or technology departments that will have to support applications that implement the solution.  The y-axis show stage proximity to these stakeholders.\n",
    "\n",
    "While every project is different, most stages use a similar proportion of time.  The horizontal axis lays-out the timeline.\n",
    "\n",
    "![machine learning process](images/ml_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstation\n",
    "\n",
    "The following code demonstrates the programming portions of these stages and steps using a toy example on a simulated diverse dataset of both numeric and categorical data.  This does display the important steps that must be taken with stakeholders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that extracts features from the data then creates a model\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# make data (both numeric and categorical)\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "# generate 2d classification dataset\n",
    "X_cat, not_used = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)#; make_classification(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "# generate regression dataset\n",
    "X_num, y_num = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ed2f934815d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Encode y and combine datasets, X y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'survived'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'survived'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#Encode y and combine datasets, X y\n",
    "X = data.drop('survived', axis=1)\n",
    "y = data['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# create feature union of numeric data\n",
    "features = []\n",
    "features.append(('pca', PCA()))\t\t\t#<<<-grid\n",
    "features.append(('select_best', SelectKBest(k=6))) #<<<-grid\n",
    "num_feature_eng = FeatureUnion(features)\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_features = ['age', 'fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "\t('num_feature_eng', feature_eng)\n",
    "\t])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "\t('select_best', SelectKBest(k=6))\n",
    "\t])\n",
    "\t\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'pca': ('n_components', [.75, .80, .85, .90, .95]), \n",
    "\t\t'select_best':('k',[5, 7, 9, 11]) \n",
    "\t\t}\n",
    "gridClf = GridSearchCV(clf, grid, cv=5)\n",
    "\n",
    "\n",
    "# Learning curve\n",
    "from sklearn.learning_curve import learning_curve\n",
    "title = 'Learning Curves (SVM, linear kernel, $\\gamma=%.6f$)' %classifier.best_estimator_.gamma\n",
    "estimator = SVC(kernel='linear', gamma=classifier.best_estimator_.gamma)\n",
    "plot_learning_curve(estimator, title, X_train, y_train, cv=cv)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Final model evaluation\n",
    "classifier.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
